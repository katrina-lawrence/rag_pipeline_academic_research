{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Builing a RAG Pipeline\n",
    "The purpose of this project is to build a RAG pipeline in order to query academic documents. In particular, we will begin to look at math and machine learning research. This project will go through the following steps:\n",
    "\n",
    "1) installing the required libraries\n",
    "2) preparing the knowledge base\n",
    "3) creating embeddings for the knowledge base\n",
    "4) encoding the user query\n",
    "5) retrieving relevant documents\n",
    "6) combining the query with the retrived document\n",
    "7) generating a response using GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Knowledge Base\n",
    "Creating a list of strings. Ideally want to find academic papers on math and machine learning topics as the knowledge base.             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example knowledge base\n",
    "knowledge_base = [\n",
    "    \"The Tesla Model S has an estimated range of up to 370 miles on a single charge.\",\n",
    "    \"The Tesla Model 3 is a more affordable option with a range of up to 350 miles.\",\n",
    "    \"The Tesla Model X offers a range of around 340 miles and features falcon-wing doors.\",\n",
    "    \"Tesla's autopilot system is a suite of advanced driver-assistance system features.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Embeddings for the Knowledge Base\n",
    "sentence-transformers will be used to generate the embeddings for the documents in the knowledge base. Sentence-transformers contain pre-trained embedding (encoding) models, this provides flexibility. This project will use the 'all-MiniLM-L6-v2' model, which has been specifically designed for sentence embeddings. It is based on nthe MiniML architecture which is a smaller version of the BERT model; it is 6 layers deep. Because of its samller size, it is more computationally efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lawre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lawre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lawre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lawre\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\lawre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#loading a pre-trained model for sentence embedding\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#generating the embeddings of the knowledgebase\n",
    "kb_embeddings = embedder.encode(knowledge_base, convert_to_tensor = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the User Query\n",
    "In order for the model to ingest the query, it must be embedded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example query\n",
    "query = \"What is the range of the Tesla Model S\"\n",
    "\n",
    "#embedding the query\n",
    "query_embedding = embedder.encode(query, convert_to_tensor = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the Relevant Documents\n",
    "Using the cosine similarity metric to find the most relevant document(s) from the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Document: The Tesla Model S has an estimated range of up to 370 miles on a single charge.\n"
     ]
    }
   ],
   "source": [
    "#calculting the cosine similarity between the query embedding and the knowledge base embeddings\n",
    "cosine_scores = torch.nn.functional.cosine_similarity(query_embedding, kb_embeddings)\n",
    "\n",
    "### retrieving the top 1 most similar document ###\n",
    "\n",
    "#finding the pair find with best cosine score\n",
    "top_k = torch.topk(cosine_scores, k=1)\n",
    "\n",
    "#getting the index of the document with the best cosine score\n",
    "retrieved_doc_idx = top_k.indices[0].item()\n",
    "\n",
    "#retrieving the document using the index from the above step\n",
    "retrieved_doc = knowledge_base[retrieved_doc_idx]\n",
    "\n",
    "#printing the retrieved doc\n",
    "print(f\"Retrieved Document: {retrieved_doc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the Query and the Retrieved Document\n",
    "This step is necessary to provide more context for the generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Query: What is the range of the Tesla Model S\\nContext: The Tesla Model S has an estimated range of up to 370 miles on a single charge.\\nAnswer:'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_input = f\"Query: {query}\\nContext: {retrieved_doc}\\nAnswer:\"\n",
    "combined_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative a Response using GPT-2\n",
    "Using this pre-trained model to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the pre-trained models\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Document: The Tesla Model S has an estimated range of up to 370 miles on a single charge.\n",
      "Generated Response: Query: What is the range of the Tesla Model S\n",
      "Context: The Tesla Model S has an estimated range of up to 370 miles on a single charge.\n",
      "Answer: The range of a Tesla Model X is based on the range and weight of the vehicle.\n",
      "The range of an electric vehicle is based upon the weight of its battery pack.\n",
      "Tesla Model S is a vehicle that is designed to be driven on a highway.\n",
      "It is designed for use in a variety of situations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#encoding the combined input\n",
    "input_ids = tokenizer.encode(combined_input, return_tensors = 'pt')\n",
    "\n",
    "#set the attention mask\n",
    "attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "\n",
    "#generating the response\n",
    "# Generate a response with adjusted generation parameters\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=100,                 # Increased max length for a longer response\n",
    "    num_return_sequences=1,         # Return only one sequence\n",
    "    pad_token_id=tokenizer.eos_token_id,   # Use EOS token for padding\n",
    "    temperature=0.3,                # Lower temperature for more focused output\n",
    "    top_p=0.8,                      # Nucleus sampling for more coherence\n",
    "    top_k=40,                       # Use top-k sampling to reduce unlikely words\n",
    "    no_repeat_ngram_size=3          # Avoid repetitive phrases\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens= True)\n",
    "\n",
    "#printing the response\n",
    "print(f\"Retrieved Document: {retrieved_doc}\")\n",
    "print(f\"Generated Response: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
