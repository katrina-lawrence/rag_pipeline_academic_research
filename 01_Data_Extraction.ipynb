{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction\n",
    "Often times, we want to extract or scrape data from online resources of PDFs. The purpose of this project is to generate concise summaries for long documents or reports. This first step of data extraction is critical to obtain the text and manipulate it in such a way that the python program can read it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from an Online Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installations\n",
    "#!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sign up Sign in Sign up Sign in Chia Jeng Yang Follow WhyHow.AI -- 1 Listen Share We have a structural problem in AI. While LLM and AI systems are getting better and better, one problem still stands out in particular — context injection. Context can come from: Implementations of Vector-Only RAG systems today are a ‘One-Way’ street. The common implementation pattern is a multi-agent system on top of a vector database so natural language queries can retrieve the right information for the LLM to construct an answer. This data typically from documents and databases can tell us what the data is, but not how the data should be used. For example, if we discover that the knowledge base contains an outdated fact i.e. “That manual should only be used for customers in Asia”, that information or expert knowledge cannot be easily considered by a Vector RAG system, the way that humans can. This type of problem space around expert knowledge infra is what we’re building up to. RAG is just the starting point of KGs. The point of a schema is that it bridges the gap between how the LLM and a human expert interprets data. This allows us to create ‘Two-Way’ RAG systems that allow your enterprises to capture and store expert feedback about how your information should be interpreted, and used moving forward. A KG RAG system can then be a two-way street where the human can use natural language to directly contribute to the KG memory in ways that cannot really work for a vector RAG system, which can be viewed as a one-way retrieval system. ‘Memory’ in vector systems is just adding additional embeddings for retrieval, and not in any way affecting the meaning and status of the existing embeddings. The fact that the graph is also human readable is an additional benefit. We want to map implicit knowledge, not in an arbitrary way, but in a way that ties the expert feedback to your existing knowledge base. Put another way, expert feedback is always tied to the context and the knowledge base you are using. The value of expert feedback is then only enabled if you are able to easily capture it and systematically tie it back to the context, not simply throw it into a knowledge base where it cannot be reliably retrieved at the right time. In order to make sure that we can systematically tie expert feedback back into a knowledge base, we need to make sure that the language that experts use, can be systematically tied back into how the LLM interprets and stores data within memory. Schemas are relevant as the structural medium that allows both the LLM and experts to ‘speak’ the same precise language. Schemas are precise representations that help direct the feedback to the particular knowledge where the feedback should be stored beside. For example, in the case of a RAG system around manufacturing manuals, we can imagine that there can be a disconnect between stored digital knowledge from static documents, and the realities of how the physical system works. Being able to capture how that difference in real time through human feedback is important. The schema helps determine how exactly the schema should affect the knowledge base. These RAG systems are then not just valuable in itself, but can serve as a gateway to interact with employees and collect up-to-date information and employee heuristics about how such information should be utilized. This example above shows how you can give feedback to a Knowledge Graph RAG system across a range of dimensions: Although there are ways for us to change and tune the embeddings through fine-tuning, scenarios that rely on RAG are sensitive to the real-time nature of the knowledge base. When we are building a RAG system instead of a model, we are already making a decision that fine-tuning against a dynamic data-set is not worth the effort/reward. As such, it would be difficult to make the argument that fine-tuning expert feedback (which tends to be very specific, and related to specific parts of the knowledge base, as opposed to generally applicable) would be a practical alternative. Instead of therefore having a binary approach between a static knowledge base (pre-LLMs), and fine-tuning a model (expensive and difficult against dynamic data-sets), a third option emerges, which is about creating an intermediary semantic layer between the expert, vector embeddings, and the LLM that allow for expert feedback to intelligently amend the knowledge graph representation that sits between the vector embeddings and the LLM. This is not something that can only be done from the ground up. We do not intend to mean that every system must only have a graph or that we need a schema that has to be fully defined before feedback can be introduced (although that is the most ideal set-up). There are a number of ways this schema can be built up iteratively. For example, we can start with a vector database and a sparse schema (i.e. a document graph), and then build more granular schemas where needed. We can also iteratively build schemas based on questions, answers, and feedback. This process involves using LLMs to build a schema to store relevant questions, its answers, and the feedback provided, with the initial information coming from vector RAG. It is far easier to build these KG systems (with Hybrid Vector RAG) from the ground up, but if you have an existing Vector RAG system and are interested in augmenting your existing systems with a graph representation on top, hit us up as we are doing some R&D here. WhyHow.AI’s Knowledge Graph Studio Platform (currently in Beta) is the easiest way to build modular, agentic Knowledge Graphs, combining workflows from LLMs, developers and non-technical domain experts. If you’re thinking about, in the process of, or have already incorporated knowledge graphs in RAG for accuracy, memory and determinism, we’d love to chat at team@whyhow.ai, or follow our newsletter at WhyHow.AI. Join our discussions about rules, determinism and knowledge graphs in RAG on our Discord. -- -- 1 WhyHow.AI Co-Founder of WhyHow.AI Help Status About Careers Press Blog Privacy Terms Text to speech Teams\n"
     ]
    }
   ],
   "source": [
    "#Function to extract text from a web page\n",
    "def extract_text_from_url(url):\n",
    "    \n",
    "    #sending a request to fetch the web page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    #need to check is the request was successful\n",
    "    if response.status_code == 200:\n",
    "        #parse the page content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        #extract the main content from the article (usually inside <p> tags)\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        #combine the text from all paragraphs\n",
    "        article_text = ' '.join([p.get_text() for p in paragraphs])\n",
    "        \n",
    "        #returning the text from the article as a string\n",
    "        return article_text\n",
    "\n",
    "    else:\n",
    "        return \"Failed to retrieve the webpage.\"\n",
    "\n",
    "\n",
    "#Example usage\n",
    "url = \"https://medium.com/enterprise-rag/dynamic-expert-feedback-kg-rag-as-a-two-way-retrieval-memory-system-vs-one-way-vector-system-63b28c674336\"\n",
    "\n",
    "article_text = extract_text_from_url(url)\n",
    "\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sign up Sign in Sign up Sign in Chia Jeng Yang Follow WhyHow.AI -- 1 Listen Share We have a structural problem in AI. While LLM and AI systems are getting better and better, one problem still stands out in particular — context injection. Context can come from: Implementations of Vector-Only RAG systems today are a ‘One-Way’ street. The common implementation pattern is a multi-agent system on top of a vector database so natural language queries can retrieve the right information for the LLM to construct an answer. This data typically from documents and databases can tell us what the data is, but not how the data should be used. For example, if we discover that the knowledge base contains an outdated fact i.e. “That manual should only be used for customers in Asia”, that information or expert knowledge cannot be easily considered by a Vector RAG system, the way that humans can. This type of problem space around expert knowledge infra is what we’re building up to. RAG is just the starting point of KGs. The point of a schema is that it bridges the gap between how the LLM and a human expert interprets data. This allows us to create ‘Two-Way’ RAG systems that allow your enterprises to capture and store expert feedback about how your information should be interpreted, and used moving forward. A KG RAG system can then be a two-way street where the human can use natural language to directly contribute to the KG memory in ways that cannot really work for a vector RAG system, which can be viewed as a one-way retrieval system. ‘Memory’ in vector systems is just adding additional embeddings for retrieval, and not in any way affecting the meaning and status of the existing embeddings. The fact that the graph is also human readable is an additional benefit. We want to map implicit knowledge, not in an arbitrary way, but in a way that ties the expert feedback to your existing knowledge base. Put another way, expert feedback is always tied to the context and the knowledge base you are using. The value of expert feedback is then only enabled if you are able to easily capture it and systematically tie it back to the context, not simply throw it into a knowledge base where it cannot be reliably retrieved at the right time. In order to make sure that we can systematically tie expert feedback back into a knowledge base, we need to make sure that the language that experts use, can be systematically tied back into how the LLM interprets and stores data within memory. Schemas are relevant as the structural medium that allows both the LLM and experts to ‘speak’ the same precise language. Schemas are precise representations that help direct the feedback to the particular knowledge where the feedback should be stored beside. For example, in the case of a RAG system around manufacturing manuals, we can imagine that there can be a disconnect between stored digital knowledge from static documents, and the realities of how the physical system works. Being able to capture how that difference in real time through human feedback is important. The schema helps determine how exactly the schema should affect the knowledge base. These RAG systems are then not just valuable in itself, but can serve as a gateway to interact with employees and collect up-to-date information and employee heuristics about how such information should be utilized. This example above shows how you can give feedback to a Knowledge Graph RAG system across a range of dimensions: Although there are ways for us to change and tune the embeddings through fine-tuning, scenarios that rely on RAG are sensitive to the real-time nature of the knowledge base. When we are building a RAG system instead of a model, we are already making a decision that fine-tuning against a dynamic data-set is not worth the effort/reward. As such, it would be difficult to make the argument that fine-tuning expert feedback (which tends to be very specific, and related to specific parts of the knowledge base, as opposed to generally applicable) would be a practical alternative. Instead of therefore having a binary approach between a static knowledge base (pre-LLMs), and fine-tuning a model (expensive and difficult against dynamic data-sets), a third option emerges, which is about creating an intermediary semantic layer between the expert, vector embeddings, and the LLM that allow for expert feedback to intelligently amend the knowledge graph representation that sits between the vector embeddings and the LLM. This is not something that can only be done from the ground up. We do not intend to mean that every system must only have a graph or that we need a schema that has to be fully defined before feedback can be introduced (although that is the most ideal set-up). There are a number of ways this schema can be built up iteratively. For example, we can start with a vector database and a sparse schema (i.e. a document graph), and then build more granular schemas where needed. We can also iteratively build schemas based on questions, answers, and feedback. This process involves using LLMs to build a schema to store relevant questions, its answers, and the feedback provided, with the initial information coming from vector RAG. It is far easier to build these KG systems (with Hybrid Vector RAG) from the ground up, but if you have an existing Vector RAG system and are interested in augmenting your existing systems with a graph representation on top, hit us up as we are doing some R&D here. WhyHow.AI’s Knowledge Graph Studio Platform (currently in Beta) is the easiest way to build modular, agentic Knowledge Graphs, combining workflows from LLMs, developers and non-technical domain experts. If you’re thinking about, in the process of, or have already incorporated knowledge graphs in RAG for accuracy, memory and determinism, we’d love to chat at team@whyhow.ai, or follow our newsletter at WhyHow.AI. Join our discussions about rules, determinism and knowledge graphs in RAG on our Discord. -- -- 1 WhyHow.AI Co-Founder of WhyHow.AI Help Status About Careers Press Blog Privacy Terms Text to speech Teams'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installations\n",
    "#!pip install PyPDF2 pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract text from a PDF using pdfplumber\n",
    "def extract_text_from_pdf_with_plumber(pdf_file_path):\n",
    "    with pdfplumber.open(pdf_file_path) as pdf:\n",
    "        pdf_text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            pdf_text += page.extract_text()\n",
    "        return pdf_text\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of PAN 2024:\n",
      "Multi-Author Writing Style Analysis,\n",
      "Multilingual Text Detoxification,\n",
      "Oppositional Thinking Analysis, and\n",
      "Generative AI Authorship Verification\n",
      "Condensed Lab Overview\n",
      "Abinew Ali Ayele,1 Nikolay Babakov,2 Janek Bevendorff,3\n",
      "Xavier Bonet Casals,4 Berta Chulvi,5 Daryna Dementieva,6 Ashaf Elnagar,7\n",
      "Dayne Freitag,8 Maik Fröbe,9 Damir Korenčić,10 Maximilian Mayerl,11\n",
      "Daniil Moskovskiy,12 Animesh Mukherjee,13 Alexander Panchenko,12\n",
      "Martin Potthast,14 Francisco Rangel,15 Naquee Rizwan,13 Paolo Rosso,5,16\n",
      "Florian Schneider,1 Alisa Smirnova,17 Efstathios Stamatatos,18\n",
      "Elisei Stakovskii, Benno Stein,19 Mariona Taulé,4 Dmitry Ustalov,20\n",
      "Xintong Wang,1 Matti Wiegmann,19 Seid Muhie Yimam,1 and Eva Zangerle21\n",
      "1 Universität Hamburg, Germany, 2 Universidade de Santiago de Compostela, Spain\n",
      "3 Leipzig University, Germany, 4 Universitat de Barcelona, Spain, 5 Univ. Politècnica\n",
      "de València, Spain, 6 Technical University of Munich, Germany, 7 University of\n",
      "Sharjah, United Arab Emirates, 8 SRI International, USA, 9 Friedrich Schiller\n",
      "University Jena, Germany, 10 Ruđer Bošković Institute, Croatia, 11 University of\n",
      "Applied Sciences BFI Vienna, Austria, 12 Skoltech & AIRI, Russia, 13 Indian\n",
      "Institute of Technology Kharagpur, India, 14 University of Kassel, hessian.AI, and\n",
      "ScaDS.AI, Germany 15 Symanto Research, Spain, 16 ValgrAI - Valencian Graduate\n",
      "School and Research Network of AI, Spain, 17 Toloka, Switzerland, 18 University of\n",
      "the Aegean, Greece, 19 Bauhaus-Universität Weimar, Germany, 20 JetBrains, Serbia,\n",
      "21 University of Innsbruck, Austria\n",
      "pan@webis.de pan.webis.de\n",
      "Abstract The goal of the PAN lab is to advance the state of the art\n",
      "in text forensics and stylometry through an objective evaluation of new\n",
      "and established methods on new benchmark datasets. In 2024, we or-\n",
      "ganizedfoursharedtasks:(1)multi-authorwritingstyleanalysis,which\n",
      "we continue from 2023; (2) multilingual text detoxification, a new task\n",
      "thataimstore-formulatetextinanon-toxicwayformultiplelanguages;\n",
      "(3) oppositional thinking analysis, a new task that aims to discriminate\n",
      "critical thinking from conspiracy narratives and identify their core ac-\n",
      "tors;and(4)generativeAIauthorshipverification,whichformulatesthe\n",
      "detectionofAI-generatedtextasanauthorshipproblem.PAN2024con-\n",
      "cluded as one of our most successful editions with 74 notebook papers\n",
      "by 147 participating teams.2 Ayele et al.\n",
      "1 Introduction\n",
      "PAN is a workshop series and a networking initiative for stylometry and digi-\n",
      "tal text forensics. PAN hosts computational shared tasks on authorship analy-\n",
      "sis, computational ethics, and the originality of writing. Since the workshop’s\n",
      "inception in 2007, we organized 73 shared tasks1 and assembled 57 evalua-\n",
      "tion datasets2 plus nine datasets contributed by the community. In 2024, we\n",
      "organized four tasks that concluded in 74 notebook papers by 147 participating\n",
      "teams.\n",
      "First, the Multi-Author Writing Style Analysis task asks to, given a docu-\n",
      "ment,determineatwhichpositionstheauthorchanges.Thistaskwasrevamped\n",
      "for 2023 with a new dataset and structured around topical heterogeneity as\n",
      "an indicator of difficulty. We continued the task in 2024 with minor modifica-\n",
      "tions since it attracts consistent participation of high technical quality and the\n",
      "problem is still relevant and offers room for improvements. A total of 15 teams\n",
      "submitted notebook papers to Multi-Author Writing Style Analysis. The task\n",
      "details are described in Section 2.\n",
      "Second, the new Multilingual Text Detoxification task asks to, given a toxic\n",
      "piece of text, re-write it in a non-toxic way while saving the main content as\n",
      "much as possible. The task was prepared for 9 languages—English, Spanish,\n",
      "German, Chinese, Arabic, Hindi, Ukrainian, Russian, and Amharic—and had\n",
      "cross-lingual and multilingual challenges. A total of 31 teams submitted their\n",
      "solutions to Multilingual Text Detoxification resulting in 12 notebook papers.\n",
      "The task details are described in Section 3.\n",
      "Third, the new Oppositional Thinking Analysis task asks, given an online\n",
      "message, to first distinguish between critical and conspiracy texts, and second,\n",
      "to detect the elements of the oppositional narratives. A total of 83 teams sub-\n",
      "mitted their solutions to Oppositional Thinking Analysis resulting in 18 note-\n",
      "book papers. The task details are described in Section 4.\n",
      "Fourth, the new Generative AI Authorship Verification task asks, given one\n",
      "textauthoredbyahumanandonebyamachine,topickoutthehuman-written\n",
      "one.DetectingAI-generatedtextisataskofhighurgencyand,asanauthorship\n",
      "task, it falls deeply within PAN’s expertise. We formulate AI-detection as a\n",
      "verification task and collaborate with the ELOQUENT Lab to generate a total\n",
      "of 70 different verification datasets to benchmark the PAN submissions. A total\n",
      "of 34 teams submitted to Generative AI Authorship Verification, resulting in\n",
      "29 notebook papers. The task details are described in Section 5.\n",
      "PAN is committed to reproducible research in IR and NLP, hence all par-\n",
      "ticipants are asked to submit their software (instead of just their predictions)\n",
      "through the submission software TIRA. With the recent updates to the TIRA\n",
      "platform [32], a majority of the submissions to PAN are publicly available as\n",
      "docker containers. In the following sections, we briefly outline the 2024 tasks\n",
      "and their results.\n",
      "1Find PAN’s past shared tasks at pan.webis.de/shared-tasks.html\n",
      "2Find PAN’s datasets at pan.webis.de/data.htmlOverview of PAN 2024: Condensed Lab Overview 3\n",
      "2 Multi-Author Writing Style Analysis\n",
      "Theanalysisofwritingstylesisthefoundationofauthorshipidentificationtasks.\n",
      "The multi-author writing style analysis task, as part of PAN@CLEF, continues\n",
      "to develop challenges in this crucial field of research. Over the years, the task\n",
      "has evolved significantly: from identifying and grouping individual authors [108]\n",
      "to detecting whether a document has been written by a single or multiple au-\n",
      "thors [127, 55, 146] and identifying the actual number of authors [145], and\n",
      "finally, to paragraph-level style change detection [141, 142, 143].\n",
      "In the PAN’24 multi-author writing style analysis task, participants were\n",
      "askedtoidentifyallpositionsofwritingstylechangeswithinagiventext.Specif-\n",
      "ically,foreachpairofconsecutiveparagraphs,thetaskwastocomputewhether\n",
      "there is a change in writing style between the two paragraphs. The dataset used\n",
      "for this task is split into three subsets of increasing difficulty: Easy: Each doc-\n",
      "ument contains a variety of topics, therefore, topic information can be used for\n",
      "detectingchangesinwritingstyle.Medium: Thetopicscontainedinadocument\n",
      "are more homogeneous, requiring the approaches to focus more on writing style\n",
      "to solve the detection task. Hard: The paragraphs in a document are of a single\n",
      "topic. We control for topical diversity to ensure that, particularly in the hard\n",
      "dataset, topical differences cannot be used as a proxy signal for authorship and\n",
      "that the focus remains on stylistic cues for detecting changes in writing style.\n",
      "Data Set and Evaluation\n",
      "The dataset used for the multi-author writing analysis task is based on user\n",
      "postsonReddit3.Weselectedpostsfromthefollowingsubredditstoensurethat\n",
      "avarietyoftopicsisusedforthecreationofthedatasets:r/worldnews,r/politics,\n",
      "r/askhistorians, and r/legaladvice. After extracting posts from these subreddits,\n",
      "we applied cleaning steps, such as removing quotes, whitespace, emojis, or hy-\n",
      "perlinks. The cleaned user posts were then split into paragraphs.\n",
      "To generate documents for the dataset, we used paragraphs from a single\n",
      "Reddit post to ensure minimal topical coherence between paragraphs of the\n",
      "generated document. Each document was composed of paragraphs written by\n",
      "a randomly selected number of two to four authors. For each paragraph, we\n",
      "extracted and computed semantic and stylistic feature vectors to characterize\n",
      "the paragraph. The paragraphs were then concatenated based on the similarity\n",
      "of their feature vectors. This mixing approach allowed us to control for topical\n",
      "and stylistic similarity, enabling the creation of more coherent documents and\n",
      "allowingustoadjustthedifficultyofthemulti-authorwritingstyletask.Forthe\n",
      "threedatasets,weconfiguredthesimilaritythresholdforconsecutiveparagraphs\n",
      "to be (1) relatively large for the easy dataset, (2) moderate for the medium\n",
      "dataset, and (3) small for the hard dataset. Each of the easy, medium, and hard\n",
      "datasetscontains6,000documents.Weprovidedparticipantswithtraining,test,\n",
      "and validation splits for all three datasets. The training sets contain 70% of the\n",
      "3https://www.reddit.com/4 Ayele et al.\n",
      "Table1:Overallresultsforthemulti-authoranalysistask,rankedbyaverageF\n",
      "1\n",
      "performance across all three datasets. Best results are marked in bold.\n",
      "Team Easy F Medium F Hard F\n",
      "1 1 1\n",
      "fosu-stu [80] 0.987 0.887 0.834\n",
      "nycu-nlp [68] 0.964 0.857 0.863\n",
      "no-999 [139] 0.991 0.830 0.832\n",
      "huangzhijian [50] 0.985 0.815 0.826\n",
      "text-understanding-and-analysi [46] 0.991 0.815 0.818\n",
      "bingezzzleep [135] 0.985 0.818 0.807\n",
      "openfact [63] 0.981 0.821 0.805\n",
      "chen [20] 0.968 0.822 0.807\n",
      "baker [134] 0.976 0.816 0.770\n",
      "gladiators [56] 0.956 0.809 0.783\n",
      "khaldi-abderrahmane 0.905 0.806 0.641\n",
      "karami-sh [117] 0.972 0.664 0.642\n",
      "riyahsanjesh [113] 0.825 0.712 0.599\n",
      "liuc0757 [72] 0.696 0.717 0.503\n",
      "lxflcl66666 [66] 0.606 0.455 0.484\n",
      "foshan-university-of-guangdong [73] 0.517 0.394 0.352\n",
      "Baseline Predict 1 0.466 0.343 0.320\n",
      "Baseline Predict 0 0.112 0.323 0.346\n",
      "Baseline Random 0.414 0.506 0.495\n",
      "documents in each dataset, while the test and validation sets contain 15% each.\n",
      "The test sets were withheld for the evaluation phase of the competition.\n",
      "The performance of the submitted approaches is evaluated per dataset by\n",
      "macro-averaged F1-score value across all documents.\n",
      "Results\n",
      "The task received 16 valid software submissions. The results achieved by the\n",
      "participantsareshowninTable1.ThebestaverageF acrossthethreedatasets\n",
      "1\n",
      "wasachievedbythefosu-stuteam.Fortheeasydataset,teamsno-999[139]and\n",
      "text-understanding-and-analysi [46] achieved the highest F score (0.991), for\n",
      "1\n",
      "themediumdataset,fosu-stu[80]reachedanF scoreof0.887,andforthehard\n",
      "1\n",
      "dataset, team nycu-nlp [68] achieved a F of 0.863. All submissions were able to\n",
      "1\n",
      "outperform the three simple baselines: a random baseline, one that predicted a\n",
      "style change for each pair of paragraphs, and one that predicted no style change\n",
      "foreachpairofparagraphs.Furtherdetailsontheapproachestakencanbefound\n",
      "in the overview paper [144].\n",
      "3 Multilingual Text Detoxification\n",
      "Text detoxification is a subtask of text style transfer where the style of text\n",
      "should be changed from toxic to neutral while preserving the content. As lan-Overview of PAN 2024: Condensed Lab Overview 5\n",
      "Table 2: The statistics of all ParaDetox datasets used in the TextDetox shared\n",
      "task.Thehumandetoxifiedreferenceswerecollectedeitherviacrowdsourcingor\n",
      "locally hired native speaker. For English and Russian, the previously collected\n",
      "train data was available during all shared task’s phases. For other languages,\n",
      "1000 samples per language were divided correspondingly into development and\n",
      "test parts.\n",
      "Language Source of Annotation Train Dev Test\n",
      "Toxic Samples Process\n",
      "English [53] Crowdsourcing+Manual 11939 400 600\n",
      "Russian [11, 115] Crowdsourcing+Manual 8500 400 600\n",
      "Ukrainian [16] Crowdsourcing — 400 600\n",
      "Spanish [96, 124, 97] Crowdsourcing — 400 600\n",
      "German [133, 106, 107] Manual — 400 600\n",
      "Hindi [82] Manual — 400 600\n",
      "Amharic [8, 7] Manual — 400 600\n",
      "Arabic [90, 40, 87, 89] Manual — 400 600\n",
      "Chinese [77] Manual — 400 600\n",
      "guage modeling advances, there is growing concern about the potential unin-\n",
      "tended consequences of this technology. One such concern is the possibility of\n",
      "harmful or biased texts, which could perpetuate negative stereotypes or mis-\n",
      "information [64]. This has led to a growing interest in AI safety and the need\n",
      "for approaches to mitigating these risks [17]. This presents a major challenge\n",
      "for researchers and practitioners in language model safety, who need to develop\n",
      "effective detoxification techniques that can be applied to many languages. Pre-\n",
      "viously,thefirstparallelcorpusforsuchataskwasreleasedforEnglish[75]and\n",
      "Russian [27] that built a foundation for the RUSSE-2022 Text Detoxification\n",
      "shared task.\n",
      "In PAN 2024, we extend our data and challenges even to more languages.\n",
      "The participants were asked to develop text detoxification systems for 9 lan-\n",
      "guages: English, Spanish, German, Chinese, Arabic, Hindi, Ukrainian, Russian,\n",
      "and Amharic. For each language, the prepared dataset was split into two parts:\n",
      "(i)developmentand(ii)test.Forthetrainpart,wedidnotprovideanytraining\n",
      "data except for English and Russian that was publicly available from the previ-\n",
      "ous work [75, 27, 26]. Thus, in the shared task, the participants were asked to\n",
      "do experiments in two setups:\n",
      "– Cross-lingualsetup:Inthedevelopment phase,participantswereprovided\n",
      "400toxicsentencespereachlanguage.Theyhavetoexperimentwithvarious\n",
      "techniques for cross-lingual detoxification.\n",
      "– Multilingual setup: Then, in the test phase, we released parallel dev data\n",
      "andaskedparticipantstoperformdetoxificationon600samplesperlanguage\n",
      "(3600instancesintotal).Atthisphase,participantswereabletoutilizepar-6 Ayele et al.\n",
      "allel training corpora to improve their approaches and perform multilingual\n",
      "detoxification for any subset of languages.\n",
      "For both phases, an automatic leaderboard was open to provide the partici-\n",
      "pants scores of the adequacy and the proximity to the human references of their\n",
      "outputs. However, the final leaderboard was based on a human evaluation with\n",
      "crowdsourcing of subsamples from the test dataset. The human judgment gave\n",
      "a fair assessment of responses and prevented participants from over-tuning on\n",
      "automated metrics.\n",
      "Data Set and Evaluation\n",
      "Multilingual ParaDetox for 9 languages The full picture of the collected Pa-\n",
      "raDetoxdataforalltargetlanguagesispresentedinTable2.Whilethemethods\n",
      "of collecting human annotations vary across languages—some data were gath-\n",
      "ered via crowdsourcing, others by hiring local native speakers—the quality of\n",
      "the texts was uniformly verified by experts to ensure three key attributes as\n",
      "introduced in [28, 75]: (i) the style of new paraphrases is genuinely non-toxic,\n",
      "(ii) the main content is preserved, and (iii) the new texts are fluent.\n",
      "For each language for the shared task’s phases:\n",
      "– During the development phase: 400 only toxic parts were available for par-\n",
      "ticipants to perform cross-lingual experiments.\n",
      "– During the test phase: (i) 400 ParaDetox instances were fully released; (ii)\n",
      "participants should provide their final solutions for 600 toxic parts of the\n",
      "test dataset.\n",
      "For English and Russian during all phases, additional training parallel\n",
      "datasets were available from previous work [75, 27, 26]. All the data is avail-\n",
      "able online for public usage.4\n",
      "Automatic Evaluation For both phases, we provided the leaderboard based\n",
      "on an automatic evaluation setup. We evaluate the outputs based on three\n",
      "parameters—style of text, content preservation, and conformity to human\n",
      "references—combining them into the final Joint score:\n",
      "– StyleTransferAccuracy(STA)ensuresthatthegeneratedtextisindeed\n",
      "morenon-toxic.ItwasestimatedwithXLM-R[22]largeinstancefine-tuned\n",
      "forthebinarytoxicityclassificationtaskforourtargetlanguages.Themodel\n",
      "determined the degree of non-toxicity in the texts.\n",
      "– Content Similarity (SIM) is the cosine similarity between LaBSE em-\n",
      "beddings [31] of the source texts and the generated texts.\n",
      "– Fluency (ChrF1) is used to estimate the proximity of the detoxified texts\n",
      "to human references and their fluency.\n",
      "4https://huggingface.co/textdetoxOverview of PAN 2024: Condensed Lab Overview 7\n",
      "Human Evaluation We selected 100 random original toxic samples per each\n",
      "language from the test part of our dataset and performed human evaluation via\n",
      "Tolokacrowdsourcingplatform.5 Theconceptofthehumanevaluationmirrored\n",
      "the approach used in the automatic evaluation. Each project type focused on\n",
      "assessing one of the three key qualities of detoxification; style transfer accuracy,\n",
      "content similarity, or fluency:\n",
      "– Style Transfer Accuracy: we employed a pairwise comparison between\n",
      "the original toxic text and the generated detoxified text. Participants were\n",
      "tasked with determining which text was more toxic: the left text, the right\n",
      "text, or neither.\n",
      "– Content Similarity: participants were shown pairs of texts (toxic phrase\n",
      "followedby detoxified phrase) andaskedto indicate ifthe sense was similar,\n",
      "responding with “yes” or “no”.\n",
      "– Fluency: individual sentences were evaluated for intelligibility and correct-\n",
      "ness.Annotatorscouldrespondwith“yes”,“partially”,or“no”,corresponding\n",
      "to scores of 1, 0.5, and 0, respectively. The fluency score for a text pair was\n",
      "determined by comparing the detoxified text’s score to the original. If the\n",
      "detoxified text had a higher or equal fluency score, the pair received a 1;\n",
      "otherwise, it received a 0.\n",
      "Final Joint Score (J) For both automatic and human evaluation setups, the J\n",
      "score was the aggregation of the three above metrics. The metrics STA, SIM\n",
      "and FL were subsequently combined into the final J score used for the final\n",
      "ranking of approaches. Given an input toxic text x and its output detoxified\n",
      "i\n",
      "version y , for a test set of n samples:\n",
      "i\n",
      "n\n",
      "J= 1 (cid:80) STA(y )·SIM(x ,y )·FL(x ,y ),\n",
      "n i i i i i\n",
      "i=1\n",
      "where STA(y ), SIM(x ,y ), FL(x ,y ) ∈ [0,1] for automatic and ∈ {0,1} for\n",
      "i i i i i\n",
      "human evaluation for each text detoxification output y .\n",
      "i\n",
      "We calculated all the metrics separately per each language. In the end, we\n",
      "calculated the Average score of 9 Joint scores per all languages that were used\n",
      "to compile the leaderboard.\n",
      "Results\n",
      "We received 20 submissions for the development phase leaderboard and 31 sub-\n",
      "missionsforthetestphaseleaderboard;thefinalmanuallyevaluatedleaderboard\n",
      "was based on 17 submissions who confirmed their participation in the competi-\n",
      "tion[93,130,110,149,95,78,34,123,91,63,105,102,99].Thefinalleaderboard\n",
      "based on human assessments is presented in Table 3.\n",
      "Almost all of the participants used the current SOTA LLMs, among which\n",
      "are GPT-3.5 [92] and Llama-3 [3] models; to enhance the model’s performance\n",
      "5https://toloka.ai8 Ayele et al.\n",
      "Table 3: Results of the human final evaluation of the TextDetox test phase.\n",
      "Scores are sorted by the average Joint score. Scores are sorted by the average\n",
      "Joint score across all 9 languages. Baselines are highlighted with gray, Human\n",
      "References are highlighted with green.\n",
      "Team Avg System\n",
      "Human References 0.851 Human paraphrases from our multilingual ParaDetox\n",
      "SomethingAwful 0.774 Few-shot LLaMa-3 prompting+mT0-XL\n",
      "adugeen 0.741 Fine-tuned mT0-XL with ORPO [43]\n",
      "VitalyProtasov 0.723 Preprocessing+mT0-large\n",
      "nikita.sushko 0.712 Fine-tuned mT0-XL+postprocessing\n",
      "erehulka 0.708 Few-shot LLaMa-3 prompting\n",
      "bmmikheev 0.685 Few-shot LLaMa-3 prompting+GPT-3.5 post-eval.\n",
      "mkrisnai 0.681 Few-shot GPT-3.5 prompting\n",
      "d1n910 0.654 Few-shot Kimi.AI prompting\n",
      "Yekaterina29 0.639 Fine-tuned mT5-XL\n",
      "estrella 0.576 Tree of Thought GPT3.-5 prompting\n",
      "gleb.shnshn 0.564 Zero-shot LLaMa-3-70b prompting\n",
      "Delete 0.560 Elimination of toxic keywords\n",
      "mT5 0.541 Fine-tuned mT5-XL\n",
      "shredder67 0.524 Fine-tuned mT5-XL\n",
      "razvor 0.516 Few-shot LLaMa-3 prompting\n",
      "ZhongyuLuo 0.513 Translation+BART-detox&ruT5-detox\n",
      "gangopsa 0.500 Fine-tuned T5&BART+token-level editing\n",
      "Backtranslation 0.411 Translation of data to English+BART-detox\n",
      "maryam.najafi 0.177 Mistral-7b with PPO\n",
      "dkenco 0.119 Few-shot Cotype-7b prompting\n",
      "on the task of detoxification participants tested both zero-shot and few-shot\n",
      "prompting methods. Among smaller models, there were used mT5 [137] and\n",
      "mT0 [88]—these models were usually finetuned using ad hoc filtering and data\n",
      "augmentation techniques, for instance, as RAG and backtranslation. Addition-\n",
      "ally, region-specific LLMs were also employed: Cotype-7b [86] and Kimi.AI [2].\n",
      "The majority of the participants overcame the baselines and even a couple\n",
      "of solutions outperformed human references. Still, for not-so-rich-resource lan-\n",
      "guages such as Ukrainian, Chinese, Amharic, and Hindi human detoxified para-\n",
      "phrasesremainedthegoldstandard.Atthesametime,variousexperimentsfrom\n",
      "participantsillustratethatvanillausageofLLMsforthedetoxificationtaskdoes\n",
      "notachievehighresults.Atleastmoreadvancedpromptingtechniquesandfine-\n",
      "tuningonthedownstreamtaskwithourprovideddataboostedtheperformance\n",
      "significantly achieving such interesting SOTA results.Overview of PAN 2024: Condensed Lab Overview 9\n",
      "Figure1: A Telegram text annotated with elements of oppositional narrative.\n",
      "4 Oppositional Thinking Analysis: Conspiracy Theories\n",
      "vs Critical Thinking Narratives\n",
      "Conspiracytheoriesarecomplexnarrativesthatattempttoexplaintheultimate\n",
      "causes of significant events as cover plots orchestrated by secret, powerful, and\n",
      "malicious groups [29]. A challenging aspect of identifying conspiracy with NLP\n",
      "models [109, 100, 101, 35, 62, 33] stems from the difficulty of distinguishing\n",
      "critical thinking from conspiratorial thinking in automatic content moderation.\n",
      "This distinction is vital because labeling a message as conspiratorial when it is\n",
      "only oppositional could drive those who were simply asking questions into the\n",
      "arms of the conspiracy communities.\n",
      "AtPAN2024weaimatanalyzingoppositionalthinking,andmoreconcretely,\n",
      "at discriminating conspiracy from critical narratives from a stylometry perspec-\n",
      "tive.ThetaskwilladdresstwonewchallengesfortheNLPresearchcommunity:\n",
      "(1) to distinguish the conspiracy narrative from other oppositional narratives\n",
      "that do not express a conspiracy mentality (i.e., critical thinking); and (2) to\n",
      "identify in online messages the key elements of a narrative that fuels the inter-\n",
      "group conflict in oppositional thinking. Accordingly, we propose two sub-tasks:\n",
      "– Subtask 1 is a binary classification task differentiating between (1) critical\n",
      "messages that question major decisions in the public health domain, but\n",
      "do not promote a conspiracist mentality; and (2) messages that view the\n",
      "pandemic or public health decisions as a result of a malevolent conspiracy\n",
      "by secret, influential groups.\n",
      "– Subtask2isatoken-levelclassificationtaskaimedatrecognizingtextspans\n",
      "corresponding to the key elements of oppositional narratives. Since conspir-\n",
      "acynarrativesareaspecialkindofcausalexplanation,wedevelopedaspan-\n",
      "level annotation scheme that identifies the goals, effects, agents, and the\n",
      "groups-in-conflict in these narratives.\n",
      "For the second task, a new fine-grained annotation scheme was developed\n",
      "with the goal of identifying, at the text span level, how oppositional and con-\n",
      "spiracynarrativesuseinter-groupconflict.Theannotationwasperformedforthe\n",
      "described 5,000 binary-labeledmessages per language. We identify the following\n",
      "six categories of narrative elements at the span level (see Figure 1):\n",
      "– Agents: the hidden power that pulls the strings of the conspiracy. In crit-\n",
      "ical messages, agents are actors that design the mainstream public health\n",
      "policies: Government, WHO, ...;10 Ayele et al.\n",
      "Table4:Overallresultsforsubtask1onConspiracytheoriesvsCriticalthinking\n",
      "narrativesinEnglish(EN)intermsofMatthew’scorrelationcoefficient(MMC).\n",
      "Team EN MCC Team EN MCC Team EN MCC\n",
      "IUCL 0.838 nlpln 0.784 lnr-lladrogal 0.725\n",
      "AI_Fusion 0.830 RalloRico 0.777 lnr-fanny-nuria 0.725\n",
      "SINAI 0.829 LasGarcias 0.775 MarcosJavi 0.719\n",
      "ezio 0.821 zhengqiaozeng 0.775 lnr-cla 0.716\n",
      "hinlole 0.819 ALC-UPV-JD-2 0.772 lnr-jacobant. 0.716\n",
      "Zleon 0.819 LorenaEloy 0.771 MUCS 0.716\n",
      "virmel 0.819 lnr-alhu 0.770 lnr-aina-julia 0.715\n",
      "inaki 0.814 NACKO 0.769 LaDolceVita 0.707\n",
      "yeste 0.812 paranoia-pulverizers 0.768 alopfer 0.705\n",
      "auxR 0.808 DiTana 0.765 lnr-luqrud 0.705\n",
      "Elias&Sergio 0.803 FredYNed 0.764 LNR-JoanPau 0.705\n",
      "theateam 0.803 dannuchihaxxx 0.764 lnr-carla 0.700\n",
      "trustno1 0.798 lnr-detectives 0.763 lnr-Inetum 0.698\n",
      "DSVS 0.797 TargaMarhuenda 0.761 lnr-antonio 0.685\n",
      "sail 0.796 Trainers 0.759 LluisJorge 0.678\n",
      "ojo-bes. 0.796 thetaylorswift 0.757 anselmo-team 0.672\n",
      "RD-IA-FUN 0.796 locasporlnr 0.757 lnr-pavid 0.595\n",
      "Baseline BERT 0.796 lnr-adri 0.755 LNRMADME 0.546\n",
      "aish_team 0.791 TokoAI 0.754 lnr-mariagb. 0.506\n",
      "rfenthusiasts 0.790 ede 0.753 LNR_08 0.442\n",
      "Dap_upv 0.789 lnr-verdnav 0.752 Kaprov 0.370\n",
      "oppositional_opposition 0.789 lnr-dahe 0.748 lnr_cebusqui 0.048\n",
      "RD-IA-FUN 0.789 epistemologos 0.748 jtommor 0.040\n",
      "miqarn 0.788 lucia&ainhoa 0.747 eledu 0.459\n",
      "CHEEXIST 0.787 pistacchio 0.741 david-canet 0.631\n",
      "tulbure 0.787 lnr-BraulioP. 0.739 lnr-guilty 0.659\n",
      "XplaiNLP 0.787 Marc_Coral 0.739 lnrANRI 0.755\n",
      "TheGymNerds 0.785 Ramon&Cajal 0.728 ROCurve 0.800\n",
      "– Objectives: parts of the narrative that answer the question “What is in-\n",
      "tended by the agents of the conspiracy theory or by the promoters of the\n",
      "action being criticized from a critical thinking perspective?”;\n",
      "– Consequences:partsofthenarrativethatdescribetheeffectsoftheagent’s\n",
      "actions;\n",
      "– Facilitators: the facilitators are those who collaborate with the conspira-\n",
      "tors; in critical messages, facilitators are those who implement the measures\n",
      "dictated by the authorities;\n",
      "– Campaigners: in conspiracy messages, the campaigners are the ones who\n",
      "uncover the conspiracy theory; in critical messages, campaigners are those\n",
      "who resist the enforcement of laws and health instructions; and\n",
      "– Victims: the people who are deceived into following the conspiratorial plan\n",
      "or the ones who suffer due to the decisions of the authorities.Overview of PAN 2024: Condensed Lab Overview 11\n",
      "Table5:Overallresultsforsubtask1onConspiracytheoriesvsCriticalthinking\n",
      "narrativesinSpanish(ES)intermsofMatthew’scorrelationcoefficient(MMC).\n",
      "Team ES MCC Team ES MCC Team ES MCC\n",
      "SINAI 0.742 NACKO 0.646 Ramon&Cajal 0.58\n",
      "auxR 0.720 ALC-UPV-JD-2 0.646 lnr-fanny-nur. 0.58\n",
      "RD-IA-FUN 0.702 DSVS 0.646 lnr-antonio 0.57\n",
      "Elias&Sergio 0.697 RD-IA-FUN 0.644 LluisJorge 0.56\n",
      "AI_Fusion 0.687 locasporlnr 0.643 lnr-cla 0.56\n",
      "zhengqiaozeng 0.687 DiTana 0.637 lnr-jacobant. 0.56\n",
      "virmel 0.685 lnr-BraulioPaula 0.635 lnr-pavid 0.55\n",
      "trustno1 0.684 Dap_upv 0.630 alopfer 0.55\n",
      "Zleon 0.682 TheGymNerds 0.630 LNRMADME 0.54\n",
      "ojo-bes 0.681 MUCS 0.629 lnr-carla 0.54\n",
      "tulbure 0.672 LasGarcias 0.624 LorenaEloy 0.54\n",
      "sail 0.671 lnr-dahe 0.619 CHEEXIST 0.53\n",
      "nlpln 0.668 lnr-adri 0.619 lnr-guilty 0.52\n",
      "Baseline BERT 0.668 hinlole 0.619 eledu 0.50\n",
      "pistacchio 0.667 RalloRico 0.610 lnr-mariagb. 0.49\n",
      "rfenthusiasts 0.665 lnr-aina-julia 0.61 dannuchihaxxx 0.47\n",
      "XplaiNLP 0.662 lnr-verdnav 0.61 lnr-detectives 0.40\n",
      "yeste 0.660 thetaylorswift 0.60 LNR_08 0.06\n",
      "oppositional_opposition 0.660 lnr-alhu 0.60 jtommor 0.01\n",
      "epistemologos 0.656 lnr-luqrud 0.60 lnr-Inetum 0.00\n",
      "miqarn 0.656 lnr-lladrogal 0.59 Marc_Coral 0.00\n",
      "theateam 0.655 ede 0.59 MarcosJavi -0.03\n",
      "ezio 0.653 Fred&Ned 0.59 lnr_cebusqui -0.41\n",
      "lucia&ainhoa 0.652 LaDolceVita 0.59 david-canet -0.50\n",
      "TargaMarhuenda 0.651 LNR-JoanPau 0.59 lnrANRI -0.61\n",
      "TokoAI 0.651 anselmo-team 0.58 ROCurve -0.64\n",
      "paranoia-pulver. 0.649\n",
      "Data Set and Evaluation\n",
      "For the creation of the corpus, we first manually compiled a list of 2,273 pub-\n",
      "lic Telegram channels in English and Spanish that contain oppositional non-\n",
      "mainstream views on the COVID-19 pandemic. We retrieved and filtered mes-\n",
      "sages from the channels based on a set of oppositional and conspiracy keywords\n",
      "related to COVID-19. Then the messages were cleaned by removing duplicates,\n",
      "shorttexts,andtextswithalargeproportionofnon-regularwords(suchasURLs\n",
      "andmentions).Finally,themessageswererankedusinganindexofqualitybased\n",
      "onthepropertiesofamessageanditschannel.Theindexiscomposedofseveral\n",
      "criteriacapturingtheprevalenceofCOVID-19topicsandthechannel’sactivity.\n",
      "We developed an annotation schema to differentiate between the messages\n",
      "criticizing the mainstream views on COVID-19 and the messages evoking the\n",
      "existenceofaconspiracy.Amessagewaslabeled\"conspiracy\"ifanyofthesefour\n",
      "criteriaweremet:(1)itframedCOVID-19orarelatedpublichealthstrategyas\n",
      "the result of the agency of a small and malevolent secret group; (2) it claimed\n",
      "that the pandemic is not real (e.g. a plandemic); (3) it accused critics of the12 Ayele et al.\n",
      "Table 6: Overall results for subtask 2 on the Text-span recognition of elements\n",
      "ofoppositionalnarratives,inEnglish(EN)andSpanish(ES),intermsofmacro-\n",
      "averaged span-F1\n",
      "Team EN span-F1 Team ES span-F1\n",
      "tulbure 0.6279 tulbure 0.6129\n",
      "Zleon 0.6089 Zleon 0.5875\n",
      "hinlole 0.5886 AI_Fusion 0.5777\n",
      "oppositional_opposition 0.5866 CHEEXIST 0.5621\n",
      "AI_Fusion 0.5805 virmel 0.5616\n",
      "virmel 0.5742 miqarn 0.5603\n",
      "miqarn 0.5739 DSVS 0.5529\n",
      "TargaMarhuenda 0.5701 TargaMarhuenda 0.5364\n",
      "ezio 0.5694 Elias&Sergio 0.5151\n",
      "zhengqiaozeng 0.5666 hinlole 0.4994\n",
      "Elias&Sergio 0.5627 Baseline BETO 0.4934\n",
      "DSVS 0.5598 Dap_upv 0.4914\n",
      "CHEEXIST 0.5524 zhengqiaozeng 0.4903\n",
      "rfenthusiasts 0.5479 ALC-UPV-JD-2 0.4885\n",
      "ALC-UPV-JD-2 0.5377 ezio 0.4869\n",
      "Baseline BETO 0.5323 nlpln 0.4672\n",
      "Dap_upv 0.5272 rfenthusiasts 0.4666\n",
      "aish_team 0.5213 SIANI 0.4151\n",
      "SINAI 0.4582 TheGymNerds 0.3984\n",
      "Trainers 0.3382 DiTana 0.3004\n",
      "nlpln 0.3339 ROCurve 0.2649\n",
      "ROCurve 0.2996 TokoAI 0.1878\n",
      "TokoAI 0.2760 epistemologos 0.1657\n",
      "DiTana 0.2756 LaDolceVita 0.1056\n",
      "TheGymNerds 0.2070 theateam 0.0994\n",
      "epistemologos 0.1709 oppositional_opposition 0.0037\n",
      "theateam 0.1503\n",
      "LaDolceVita 0.0726\n",
      "kaprov 0.0150\n",
      "conspiracytheoryofbeingapartoftheplot;(4)itdividedsocietyintotwo:those\n",
      "who know the truth (the conspiracy theorists) and those who remain ignorant.\n",
      "A message was labeled “critical” if it opposed publicly accepted understandings\n",
      "ofeventsbuthadnoneofthesefourcharacteristicsoftheconspiratorialmindset.\n",
      "Using this annotation scheme, 5,000 messages per language were anno-\n",
      "tated as \"conspiracy\" or \"critical\" thinking. For these messages, we performed\n",
      "anonymization by removing sensitive and identifiable information such as nick-\n",
      "names,userIDs,ande-mailaddresses.Theaveragetextlengthis128tokensfor\n",
      "Spanish texts and 265 tokens for English texts that tend to elaborate more on\n",
      "conspiracy theories.\n",
      "Eachmessagewasannotatedbythreelinguistsandtheinter-annotatoragree-\n",
      "ment (IAA) was calculated. Disagreements were discussed with the social psy-\n",
      "chologist who created the annotation scheme. For English messages, the IAA in\n",
      "terms of Krippendorf’s α is 0.79 for “conspiracy” messages and 0.60 for “criti-Overview of PAN 2024: Condensed Lab Overview 13\n",
      "cal” messages, while the average observed percentage of agreement between the\n",
      "three annotators is 91.4%, and 80.3%, respectively. For Spanish messages, Krip-\n",
      "pendorf’s α is 0.80 for “conspiracy” messages and 0.70 for “critical” messages,\n",
      "corresponding to the percentage agreements of 90.9% and 84.9%.\n",
      "For the second task, a new fine-grained annotation scheme was developed\n",
      "with the goal of identifying, at the text span level, how oppositional and con-\n",
      "spiracynarrativesuseintergroupconflict.Theannotationwasperformedforthe\n",
      "described 5,000 binary-labeled messages per language.\n",
      "Intheprocessofspan-levelannotation,eachofthe5,000SpanishandEnglish\n",
      "messageswereannotatedbytwolinguists.Currently,theannotationinstructions\n",
      "arebeingdiscussedandimprovedand,tothisend,weareusingtheGamma(γ)\n",
      "measure of the IAA test [83], yielding a first average γ of 0.43. The following\n",
      "batchhadanaveragegammaof0.53,andthelastonehadaγof0.61.Wedeemed\n",
      "this a good agreement because it is close to or above the average agreement of\n",
      "other highly conceptual span-level schemes [24, 132]. A detailed description of\n",
      "the dataset can be found in [60].\n",
      "The official evaluation metric for subtask 1 (critical vs. conspiracy classifica-\n",
      "tion)isMatthew’scorrelationcoefficient(MCC)[21],whiletheofficialmetricfor\n",
      "subtask2(span-leveldetectionofnarrativeelements)ismacro-averagedspan-F1\n",
      "[23].\n",
      "Results\n",
      "A total of 83 teams submitted their runs for subtasks 1 and 2, resulting in\n",
      "18 notebook submissions [51, 131, 44, 9, 25, 112, 4, 150, 30, 111, 36, 6, 81,\n",
      "147, 47, 128, 71]. In the tables above we illustrate the ranking per language.\n",
      "Concretely, Table 4 and Table 5 show the overall results obtained for subtask\n",
      "1 on Conspiracy theories vs critical thinking narratives, in terms of Matthew’s\n",
      "correlationcoefficient;whileTable6showstheresultsofsubtask2onText-span\n",
      "recognition of elements of oppositional narratives, in terms of macro-averaged\n",
      "span-F1.\n",
      "We will analyze in detail the results and describe the models of the partici-\n",
      "pants in the task overview paper [61].\n",
      "5 Voight-Kampff Generative AI Authorship Verification\n",
      "Authorshipverificationisafundamentaltaskinauthoridentification.Allcasesof\n",
      "questioned authorship can be decomposed into a series of verification instances,\n",
      "be it in a closed-set or open-set scenario [59]. Since PAN has been continuously\n",
      "organizing Authorship verification tasks [119, 13, 12, 118], we are well-equipped\n",
      "to tackle a timely and highly important issue: identification of machine author-\n",
      "ship in contrast to human authorship.\n",
      "Authorship identification of generative AI “in the wild” where a single doc-\n",
      "ument is disputed without reference is an open-set problem and the hardest14 Ayele et al.\n",
      "Input / Task Possible Assignment Patterns\n",
      "1. { ? , ? } 1. { A, M }\n",
      "2. { ? , ? } 2. { A, M }, { A, A }\n",
      "3. { ? , ? } −→ 3. { A, M }, { M, M }\n",
      "4. { ? , ? } 4. { A, M }, { A, A }, { M, M }\n",
      "5. { ? , ? } 5. { A, M }, { A, A }, { A, B }\n",
      "6. { ? , ? } 6. { A, M }, { A, A }, { A, B }, { M, M }\n",
      "7. ? 7. A, M\n",
      "Figure2: Hierarchy of authorship verification problems from “easiest” (1) to\n",
      "“hardest” (7), involving LLM-generated text. Ignoring mixed human and ma-\n",
      "chine authorship, the difficulty arises from the pairing constraints imposed by\n",
      "the possible assignment patterns. M denotes LLM-generated text, while A and\n",
      "B denote human-authored text (same letter meaning same human author).\n",
      "formulationofthetask.Althoughtheliteraturesuggestslimitedsuccessinsolv-\n",
      "ingthisproblemgiventhecurrentgenerationofLLMs,itisquestionablewhether\n",
      "this will remain so with improving technology. Setting aside mixed human and\n",
      "machine authorship, we have broken down all possible formulations of the prob-\n",
      "lem with increasing levels of difficulty to get a more fundamental understanding\n",
      "of the task at hand and the feasibility of potential solutions. Figure 2 visual-\n",
      "izes the cascade of all problem variants from easiest (Task 1) to most difficult\n",
      "(Task7).Intheeasiestcase,twodocumentswithunknownauthorshiparegiven,\n",
      "yetweguaranteethatexactlyoneisgeneratedbyahuman A,andtheotherby\n",
      "a machine M, respectively. This constraint is relaxed in the following variants\n",
      "where, for example, both texts may also stem from a machine, { M, M }. In the\n",
      "hardest case, a single text is given, which could be either A or M.\n",
      "For the 2024 task on “Generative AI Authorship Verification,” we follow the\n",
      "“easiest” formulation of the task in order to establish a feasibility baseline. The\n",
      "task description reads: “Given two texts, one authored by a human, one by a\n",
      "machine: pick out the human. ”\n",
      "The task is organized in collaboration with the ELOQUENT Lab [54] in\n",
      "a builder-breaker style, in which PAN participants build systems to identify\n",
      "machine authorship, while ELOQUENT participants supply datasets trying to\n",
      "break the systems.\n",
      "Data Set\n",
      "In addition to the ELOQUENT-provided data, we collected 1,359 articles of\n",
      "major 2021 U.S. news headlines from Google News. We chose this time period\n",
      "specifically as it predates the release of GPT-3.5 so that we could be reasonably\n",
      "certain the articles were actually human-authored. We used GPT-4-Turbo to\n",
      "generate a bullet-point summary of each article and the summaries were thenOverview of PAN 2024: Condensed Lab Overview 15\n",
      "Table 7: Overview of the 65 dataset variants provided as baseline datasets. All\n",
      "variants contain the same 271 human texts and (roughly) one machine gener-\n",
      "ated text per LLM used. Discarding erroenous generations, this results in 3,441\n",
      "pairings each for main and cross-domain variants, 600 for both unicode variants\n",
      "and short texts, 543 for german texts, 542 for the Kaggle prompt, 272 for both\n",
      "contrastive decoding (* using Llama2-13B).\n",
      "taCtahC\n",
      "zmoolB\n",
      "orP\n",
      "inimeG\n",
      ".pmethtiw\n",
      "nosiB-txeT\n",
      "TPG\n",
      "2amalL lartsiM\n",
      "5.1-newQ\n",
      "Variation/Obfuscation 7B 7B 0.6 0.9 002 2-OI 3.5 4 7B 70B 7B 8x7B 72B\n",
      "Main x x x x x x x x x x x x x\n",
      "Unicode sub. (machine) x x x x x x x\n",
      "Unicode sub. (both) x x x x x x x\n",
      "Cross-domain x x x x x x x x x x x x x.\n",
      "Short text x x x x x x x x x x x x x\n",
      "German text (machine) x x\n",
      "Contr. decoding (α=0.1) x*\n",
      "Contr. decoding (α=0.6) x*\n",
      "Kaggle prompt x x\n",
      "giventoaselectionof13downstreamlargelanguagemodelstowritenewarticles\n",
      "from them.\n",
      "Of the original 1,359 human-authored articles, participants were given 1,087\n",
      "together with their machine counterparts from 13 LLMs to calibrate their sys-\n",
      "tems. The remaining 272 articles and generations from 15 LLMs were kept back\n",
      "for testing, resulting in 3,984 test cases, which together form the “main” portion\n",
      "of the test set.\n",
      "To further test the robustness of the submitted systems, we generated mul-\n",
      "tiplevariantsoftheoriginalpairs.Inparticular,we:(1)amendedthepromptto\n",
      "generate German instead of English texts (this was already part of the “main”\n",
      "testset,butnotcommunicatedtotheparticipants);(2)replaced15%ofthechar-\n",
      "acters in (a) the machine texts and (b) both the human and machine texts with\n",
      "Unicode lookalike characters; (3) shuffled the test case pairs to break the topic\n",
      "coherence; (4) used contrastive decoding [121] instead of top-k/top-p sampling;\n",
      "(5) cropped texts to 35 words; and (6) used the prompt from a previous Kaggle\n",
      "competition on LLM detection [57] to generate more faithful paraphrases of the\n",
      "original articles, instead of using the stripped-down bullet point summaries.\n",
      "Intotal,wecreated65testsetvariationsfrom13(15)differentLLMs,which\n",
      "are summarized in Table 7, with ELOQUENT providing another five. A more\n",
      "detailed description is available in the joint task overview paper [15].16 Ayele et al.\n",
      "Evaluation\n",
      "At test time, participants were given pairs of human and LLM texts and had\n",
      "to calculate a score between 0 and 1, indicating which text was more likely to\n",
      "be human-authored. Scores less than 0.5 mean the left text is human and scores\n",
      "greater than 0.5 mean the right text is human. A score of exactly 0.5 could be\n",
      "giventosignalanon-decision.Weborrowedthisevaluationschemefromprevious\n",
      "installments of the PAN Authorship Verification Task.\n",
      "We rank systems by their macro-average effectiveness across all n = 70\n",
      "datasetvariants(includingELOQUENTsubmissions)discountedbyhalfastan-\n",
      "dard deviation (estimated from the scores with n−1 DoF), which penalizes un-\n",
      "stable systems that are not robust against text obfuscations or other text vari-\n",
      "ations. We use the macro average over datasets since all datasets have different\n",
      "numbers of examples, yet we consider them equally important as performance\n",
      "indicators.\n",
      "Alsoinlinewithprevioustaskinstallments,wecomputetheeffectivenessfor\n",
      "each dataset variant as the average of the established evaluation measures in\n",
      "authorship verification (all with comparable 0–1 scales). In particular:\n",
      "– Roc-Auc: The area under the Receiver Operating Characteristic curve.\n",
      "– Brier: The complement of the Brier score (mean squared loss)\n",
      "– C@1: A modified accuracy score that assigns non-answers (score = 0.5) the\n",
      "average accuracy of the remaining cases.\n",
      "– F : The harmonic mean of precision and recall.\n",
      "1\n",
      "– F : A modified F measure (precision-weighted F measure) that treats\n",
      "0.5u 0.5\n",
      "non-answers (score = 0.5) as false negatives.\n",
      "Submitted Systems\n",
      "Intotal,ourtaskattracted34teamstosubmitsystemsinadditiontothebaseline\n",
      "systems we provided. Table 8 shows the best-performing system of each team\n",
      "that submitted notebook papers and a brief description of their approach.\n",
      "Baselines Weprovidedimplementationsofsixbaselinesystemstocomparesub-\n",
      "mitted systems against four state-of-the-art zero-shot LLM detection baselines\n",
      "and two adapted authorship verification baselines.\n",
      "The zero-shot LLM detection baselines are: (1) Binoculars [42], (2) De-\n",
      "tectLLM (both NPR and LRR scoring mode), (3) DetectGPT [85], and (4)\n",
      "Fast-DetectGPT[10].AllthreewereprovidedintwovariantsusingeitherFalcon-\n",
      "7B[5]orMistral-7B[52]toestimatetextperplexities.Therequiredtextpertur-\n",
      "bations for DetectGPT and DetectLLM-NPR were generated with T5-3B [104].\n",
      "ThetwoauthorshipverificationbaselineswereadaptedtotheLLMdetection\n",
      "task by splitting each text in half and comparing the two halves against each\n",
      "other under the assumption that LLM texts are stylistically more self-similar\n",
      "than human texts. The baselines provided are a compression model (PPMd\n",
      "CBC) [114, 41] and short-text authorship unmasking [58, 14].Overview of PAN 2024: Condensed Lab Overview 17\n",
      "Table8:Thescoreisthemeanofallevaluationmeasuresacrossallothermetrics\n",
      "onthemaindatasetcorrectedbyhalfastandarddeviationtocorrectforspread.\n",
      "Team Score System\n",
      "Tavan [125] 0.924 Ensemble: LoRA-trained LLM + Binoculars\n",
      "J. Huang [46] 0.921 BERT with multiscale PU loss [126]\n",
      "Lorenz [76] 0.886 SVM with TF-IDF features\n",
      "M. Guo [39] 0.884 LSTM embeddings + GPT-2 PPL\n",
      "Z. Lin [69] 0.851 Finetuned BERT + R-Drop\n",
      "Abburi [1] 0.843 Ensemble: RoBERTa + E5 + GPT-2 Perplexity\n",
      "Miralles [84] 0.806 Entropy and text features + XGBoost\n",
      "Yadagiri [138] 0.806 Finetuned BERT + linguistic features\n",
      "Lv [79] 0.804 Finetuned DeBERTa with Reptile meta learning\n",
      "Gritsai [37] 0.796 Ensemble: LoRA-trained LLMs\n",
      "Cao [18] 0.778 Finetuned BERT\n",
      "L. Guo [38] 0.763 BERT and text features + Bi-LSTM\n",
      "Binoculars 1 0.741 Baseline Binoculars (Falcon-7B) [42]\n",
      "B. Huang [45] 0.735*Finetuned BERT + R-Drop [67]\n",
      "Valdez-Valenzuela [129] 0.727*Graph Neural Network + BERT\n",
      "Ye [140] 0.722 T5 with LM head trained to predict class\n",
      "Chen [19] 0.694 Ensemble: 2x BERT + GPT-2 (PPL)\n",
      "W. Huang [49] 0.683 Perplexity of GPT-2 trained on LLMs + SVM\n",
      "Qin [103] 0.680* Ensemble: BERTs + R-Drop\n",
      "Binoculars 2 0.671 Baseline Binoculars (Mistral-7B) [42]\n",
      "DetectLLM 1 0.654 Baseline DetectLLM LRR (Mistral-7B) [120]\n",
      "Petropoulos [98] 0.641 RoBERTa embeddings + Bi-LSTM\n",
      "Fast-DetectGPT 1 0.638 Baseline Fast-DetectGPT (Mistral-7B) [10]\n",
      "Wu [136] 0.608 BERT embeddings + extra Transformer block\n",
      "Text Length 0.604 Baseline Text length\n",
      "Z. Lin [70] 0.565 T5 with LM head trained to predict class\n",
      "Zhu [148] 0.555 Finetuned DeBERTa\n",
      "PPMd CBC 0.544 Baseline PPMd Compression-based Cosine [114, 41]\n",
      "Sun [122] 0.531 BERT embeddings + CNN\n",
      "DetectLLM 2 0.512 Baseline DetectLLM NPR (Mistral-7B) [120]\n",
      "Lei [65] 0.504 LoRA-trained ChatGLM\n",
      "Fast-DetectGPT 2 0.500 Baseline Fast-DetectGPT (Falcon-7B) [10]\n",
      "Liu [74] 0.497 Preplexity of pre-trained GPT-2\n",
      "DetectGPT 1 0.488 Baseline DetectGPT (Mistral-7B) [85]\n",
      "K. Huang [48] 0.480 Siamese DeBERTa\n",
      "DetectLLM 3 0.468 Baseline DetectLLM NPR (Falcon-7B) [120]\n",
      "Unmasking 0.467 Baseline Authorship Unmasking [58, 14]\n",
      "Sheykhlan [116] 0.460 Ensemble: BERT, RoBERTa, and Electra\n",
      "DetectLLM 4 0.460 Baseline DetectLLM LRR (Falcon-7B) [120]\n",
      "DetectGPT 2 0.439 Baseline DetectGPT (Falcon-7B) [85]\n",
      "Ostrower [94] [No software submitted]\n",
      "* Scores estimated due to run failures on some dataset variants.18 Ayele et al.\n",
      "As an additional seventh baseline, we measured and compared the text\n",
      "lengths in characters. This baseline serves as both a quasi-random baseline and\n",
      "as a data sanity check.\n",
      "Participant Systems Whileourbaselinesystemsreproduceestablishedmeth-\n",
      "ods in either authorship verification or intrinsic, zero-shot LLM detection, the\n",
      "participant systems cover a broad range of approaches. The most popular ap-\n",
      "proach is to use a BERT-based classifier with some modification (like PU loss\n",
      "or R-Drop), bagging, and/or expansion of the given training data with other\n",
      "LLM detection datasets. Some systems use engineered features like perplexity,\n",
      "properties of token distributions, or stylometrics (exclusively or in addition to\n",
      "BERT-embeddings)asclassifier(Linear,XGBoost,LSTM)inputs.Mostofthese\n",
      "classification methods apply a posterior comparison of scores similar to how we\n",
      "useBinoculars,althoughsomeparticipantsalsotrainmodelstodirectlydiscrim-\n",
      "inate between the pairings. In some cases, participants also developed zero-shot\n",
      "methods and adapted LLMs directly for the detection task, often using LoRa.\n",
      "Results\n",
      "Table 8 shows the ranking scores of the best system submitted by each partic-\n",
      "ipating team and the baselines. In total, 10 teams surpassed all baselines. The\n",
      "overall best submission (by Tavan and Najafi; mean score of 0.924) finetunes\n",
      "Mistral and Llama2 models, combining them into an ensemble with the Binoc-\n",
      "ulars baseline [42]. This approach beats the original baseline by 0.183 points,\n",
      "though there appears to be no general best strategy for AI detection. The top 5\n",
      "systemsareamixtureofzero-shotperplexityestimatorsandsupervisedblackbox\n",
      "classifiers based on BERT or even linear classifiers.\n",
      "Ontheindividualdatasets,weseethatalmostallsubmissionsperformquite\n",
      "wellonnon-obfuscatedtext(Roc-Auc>0.9).Wemustthereforeconcludethat\n",
      "eventhemostadvancedLLMsstillexhibitobviousstylisticidiosyncrasieswhich\n",
      "make their texts easy to distinguish from human ones. However, none of the\n",
      "systems is entirely robust against (unexpected) obfuscations and particularly\n",
      "short text samples are a big challenge for all systems. Some systems did not\n",
      "produce any output on the short texts due to a programming problem. For\n",
      "the final evaluation, the missing values were filled with the corresponding mean\n",
      "values from all other systems. Affected systems are marked with * in Table 8.\n",
      "A more detailed description and analysis of the submissions and the results\n",
      "can be found in the joint PAN and ELOQUENT task overview paper [15].\n",
      "Acknowledgments\n",
      "The work of Paolo Rosso, Damir Korenčić, and Berta Chulvi was in the\n",
      "framework of XAI-DisInfodemics: eXplainable AI for disinformation and con-\n",
      "spiracy detection during infodemics (MICIN PLEC2021-007681), funded byOverview of PAN 2024: Condensed Lab Overview 19\n",
      "MCIN/AEI/10.13039/501100011033 and by the European Union NextGener-\n",
      "ationEU/PRTR.\n",
      "The work from Symanto has been partially funded by XAI-DisInfodemics:\n",
      "eXplainable AI for disinformation and conspiracy detection during infodemics\n",
      "(MICIN PLEC2021-007681), Pro2Haters – Proactive Profiling of Hate Speech\n",
      "Spreaders(CDTiIDI-20210776),OBULEX-OBservatorio del Uso de Lenguage\n",
      "sEXista en la red (IVACE IMINOD/2022/106), and the ANDHI – ANomalous\n",
      "Diffusion of Harmful Information (CPP2021-008994) R&D grants.\n",
      "The work of Janek Bevendorff, Matti Wiegmann, Maik Fröbe, Martin Pot-\n",
      "thast, and Benno Stein has been funded as part of the OpenWebSearch project\n",
      "by the European Commission (OpenWebSearch.eu, GA 101070014).\n",
      "Bibliography\n",
      "[1] Abburi,H.,Pudota,N.,Veeramani,B.,Bowen,E.,Bhattacharya,S.:TeamDeloitteat\n",
      "PAN:GenerativeAITextDetection.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[2] AI,M.:Kimichatbot(2024),URLhttps://kimi.moonshot.cn,accessed:2024-05-31\n",
      "[3] AI@Meta:Llama3modelcard(2024),URL\n",
      "https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md\n",
      "[4] Albladi,A.,Seals,C.:DetectionofConspiracyvs.CriticalNarrativesandTheirElements\n",
      "usingNLP.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[5] Almazrouei,E.,Alobeidli,H.,Alshamsi,A.,Cappelli,A.,Cojocaru,R.,Hesslow,D.,\n",
      "Launay,J.,Malartic,Q.,Mazzotta,D.,Noune,B.,Pannier,B.,Penedo,G.:TheFalcon\n",
      "seriesofopenlanguagemodels.arXiv[cs.CL](28Nov2023)\n",
      "[6] Ansari,T.,Ghazi,T.,Alvi,F.,Samad,A.:DecodingCOVID-19Narratives:Conspiracyor\n",
      "Critique?WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[7] Ayele,A.A.,Dinter,S.,Belay,T.D.,Asfaw,T.T.,Yimam,S.M.,Biemann,C.:The5Jsin\n",
      "Ethiopia:AmharichatespeechdataannotationusingTolokaCrowdsourcingPlatform.In:\n",
      "Proceedingsofthe4thInternationalConferenceonInformationandCommunication\n",
      "TechnologyforDevelopmentforAfrica(ICT4DA),pp.114–120,BahirDar,Ethiopia(2022),\n",
      "URLhttps://ieeexplore.ieee.org/document/9971189\n",
      "[8] Ayele,A.A.,Yimam,S.M.,Belay,T.D.,Asfaw,T.,Biemann,C.:ExploringAmharichate\n",
      "speechdatacollectionandclassificationapproaches.In:Proceedingsofthe14th\n",
      "InternationalConferenceonRecentAdvancesinNaturalLanguageProcessing,(Sep2023),\n",
      "URLhttps://aclanthology.org/2023.ranlp-1.6\n",
      "[9] Balasundaram,P.,Swaminathan,K.,Sampath,O.,Km,P.:OppositionalThinkingAnalysis:\n",
      "ConspiracyTheoriesvsCriticalThinkingNarratives.WorkingNotesofCLEF2024,\n",
      "CEUR-WS.org(2024)\n",
      "[10] Bao,G.,Zhao,Y.,Teng,Z.,Yang,L.,Zhang,Y.:Fast-DetectGPT:Efficientzero-shot\n",
      "detectionofmachine-generatedtextviaconditionalprobabilitycurvature.arXiv[cs.CL]\n",
      "(8Oct2023)\n",
      "[11] Belchikov,A.:Russianlanguagetoxiccomments.\n",
      "https://www.kaggle.com/blackmoon/russian-language-toxic-comments(2019),accessed:\n",
      "2023-12-14\n",
      "[12] Bevendorff,J.,Chulvi,B.,laPeñaSarracén,G.L.D.,Kestemont,M.,Manjavacas,E.,\n",
      "Markov,I.,Mayerl,M.,Potthast,M.,Rangel,F.,Rosso,P.,Stamatatos,E.,Stein,B.,\n",
      "Wiegmann,M.,Wolska,M.,Zangerle,E.:OverviewofPAN2021:Authorshipverification,\n",
      "profilinghatespeechspreadersontwitter,andstylechangedetection.In:ExperimentalIR\n",
      "MeetsMultilinguality,Multimodality,andInteraction-12thInternationalConferenceofthe\n",
      "CLEFAssociation,CLEF2021,Springer(2021)\n",
      "[13] Bevendorff,J.,Ghanem,B.,Giachanou,A.,Kestemont,M.,Manjavacas,E.,Markov,I.,\n",
      "Mayerl,M.,Potthast,M.,Pardo,F.M.R.,Rosso,P.,Specht,G.,Stamatatos,E.,Stein,B.,\n",
      "Wiegmann,M.,Zangerle,E.:OverviewofPAN2020:Authorshipverification,celebrity\n",
      "profiling,profilingfakenewsspreadersontwitter,andstylechangedetection.In:\n",
      "ExperimentalIRMeetsMultilinguality,Multimodality,andInteraction-11thInternational\n",
      "ConferenceoftheCLEFAssociation,CLEF2020,Thessaloniki,Greece,September22-25,\n",
      "2020,Springer(2020)\n",
      "[14] Bevendorff,J.,Stein,B.,Hagen,M.,Potthast,M.:Generalizingunmaskingforshorttexts.\n",
      "In:Proceedingsofthe2019ConferenceoftheNorth,pp.654–659,Associationfor\n",
      "ComputationalLinguistics,Stroudsburg,PA,USA(2019),\n",
      "https://doi.org/10.18653/v1/n19-106820 Ayele et al.\n",
      "[15] Bevendorff,J.,Wiegmann,M.,Karlgren,J.,Dürlich,L.,Gogoulou,E.,Talman,A.,\n",
      "Stamatatos,E.,Potthast,M.,Stein,B.:Overviewofthe“Voight-Kampff” GenerativeAI\n",
      "AuthorshipVerificationTaskatPANandELOQUENT2024.WorkingNotesofCLEF2024,\n",
      "CEURWorkshopProceedings(2024)\n",
      "[16] Bobrovnyk,K.:Automatedbuildingandanalysisofukrainiantwittercorpusfortoxictext\n",
      "detection.In:COLINS2019.VolumeII:Workshop(2019),URLhttps://ena.lpnu.ua:\n",
      "8443/server/api/core/bitstreams/c4c645c1-f465-4895-98dd-765f862cf186/content\n",
      "[17] Brundage,M.,Avin,S.,Clark,J.,Toner,H.,Eckersley,P.,Garfinkel,B.,Dafoe,A.,Scharre,\n",
      "P.,Zeitzoff,T.,Filar,B.,Anderson,H.S.,Roff,H.,Allen,G.C.,Steinhardt,J.,Flynn,C.,\n",
      "hÉigeartaigh,S.Ó.,Beard,S.,Belfield,H.,Farquhar,S.,Lyle,C.,Crootof,R.,Evans,O.,\n",
      "Page,M.,Bryson,J.,Yampolskiy,R.,Amodei,D.:Themalicioususeofartificial\n",
      "intelligence:Forecasting,prevention,andmitigation.CoRRabs/1802.07228(2018)\n",
      "[18] Cao,H.,Han,Z.,Ye,J.,Liu,B.,Han,Y.:EnhancingHuman-MachineAuthorship\n",
      "DiscriminationinGenerativeAIVerificationTaskwithBERTandAugmentedData.\n",
      "WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[19] Chen,J.,Kong,L.:IntegratingDualBERTModelsandCausalLanguageModelsfor\n",
      "EnhancedDetectionofMachine-GeneratedTexts.WorkingNotesofCLEF2024,\n",
      "CEUR-WS.org(2024)\n",
      "[20] Chen,Z.,Han,Y.,Yi,Y.:TeamchenatPAN:IntegratingR-DropandPre-trained\n",
      "LanguageModelforMulti-authorWritingStyleAnalysis.WorkingNotesofCLEF2024,\n",
      "CEUR-WS.org(2024)\n",
      "[21] Chicco,D.,Tötsch,N.,Jurman,G.:TheMatthewscorrelationcoefficient(MCC)ismore\n",
      "reliablethanbalancedaccuracy,bookmakerinformedness,andmarkednessintwo-class\n",
      "confusionmatrixevaluation.BioDataMining14(1),13(Feb2021),ISSN1756-0381,\n",
      "https://doi.org/10.1186/s13040-021-00244-z\n",
      "[22] Conneau,A.,Khandelwal,K.,Goyal,N.,Chaudhary,V.,Wenzek,G.,Guzmán,F.,Grave,\n",
      "E.,Ott,M.,Zettlemoyer,L.,Stoyanov,V.:Unsupervisedcross-lingualrepresentation\n",
      "learningatscale.Proceedingsofthe58thACL,ACL(2020),\n",
      "https://doi.org/10.18653/V1/2020.ACL-MAIN.747\n",
      "[23] DaSanMartino,G.,Barrón-Cedeño,A.,Wachsmuth,H.,Petrov,R.,Nakov,P.:\n",
      "SemEval-2020Task11:DetectionofPropagandaTechniquesinNewsArticles.In:\n",
      "ProceedingsoftheFourteenthWorkshoponSemanticEvaluation,pp.1377–1414,\n",
      "InternationalCommitteeforComputationalLinguistics,Barcelona(online)(2020),\n",
      "https://doi.org/10.18653/v1/2020.semeval-1.186,URL\n",
      "https://aclanthology.org/2020.semeval-1.186\n",
      "[24] DaSanMartino,G.,Yu,S.,Barrón-Cedeño,A.,Petrov,R.,Nakov,P.:Fine-Grained\n",
      "AnalysisofPropagandainNewsArticles.In:Proceedingsofthe2019Conferenceon\n",
      "EmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJoint\n",
      "ConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.5636–5646,Association\n",
      "forComputationalLinguistics,HongKong,China(Nov2019),\n",
      "https://doi.org/10.18653/v1/D19-1565,URLhttps://aclanthology.org/D19-1565\n",
      "[25] Damian,S.,Herrera-Gonzalez,B.,Vazquez-Santana,D.,Calvo,H.,Felipe-Riverón,E.,\n",
      "Yáñez-Márquez,C.:DSVSatPAN2024:EnsembleApproachofLargeLanguageModelsfor\n",
      "AnalyzingConspiracyTheoriesAgainstCriticalThinkingNarratives.WorkingNotesof\n",
      "CLEF2024,CEUR-WS.org(2024)\n",
      "[26] Dementieva,D.,Babakov,N.,Panchenko,A.:Multiparadetox:Extendingtextdetoxification\n",
      "withparalleldatatonewlanguages.arXivpreprintarXiv:2404.02037(2024)\n",
      "[27] Dementieva,D.,Logacheva,V.,Nikishina,I.,Fenogenova,A.,Dale,D.,Krotova,I.,\n",
      "Semenov,N.,Shavrina,T.,Panchenko,A.:RUSSE-2022:FindingsoftheFirstRussian\n",
      "DetoxificationSharedTaskBasedonParallelCorpora.COMPUTATIONALLINGUISTICS\n",
      "ANDINTELLECTUALTECHNOLOGIES(2022),URL\n",
      "https://api.semanticscholar.org/CorpusID:253169495\n",
      "[28] Dementieva,D.,Ustyantsev,S.,Dale,D.,Kozlova,O.,Semenov,N.,Panchenko,A.,\n",
      "Logacheva,V.:Crowdsourcingofparallelcorpora:thecaseofstyletransferfor\n",
      "detoxification.Proceedingsofthe2ndCrowdScienceWorkshop:Trust,Ethics,and\n",
      "ExcellenceinCrowdsourcedDataManagementatScaleco-locatedwith47thInternational\n",
      "ConferenceonVeryLargeDataBases(VLDB2021),CEURWorkshopProceedings(2021),\n",
      "URLhttps://ceur-ws.org/Vol-2932/paper2.pdf\n",
      "[29] Douglas,K.M.,Sutton,R.M.:Whatareconspiracytheories?adefinitionalapproachtotheir\n",
      "correlates,consequences,andcommunication.AnnualReviewofPsychology74(1),271–298\n",
      "(2023),URLhttps://doi.org/10.1146/annurev-psych-032420-031329\n",
      "[30] Espinosa,D.,Sidorov,G.,Ricárdez-Vázquez,E.:UsingBERTtoIdentifyConspiracy\n",
      "Theories.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[31] Feng,F.,Yang,Y.,Cer,D.,Arivazhagan,N.,Wang,W.:Language-agnosticBERTsentence\n",
      "embedding.Proceedingsofthe60thACL,ACL(2022),\n",
      "https://doi.org/10.18653/V1/2022.ACL-LONG.62\n",
      "[32] Fröbe,M.,Wiegmann,M.,Kolyada,N.,Grahm,B.,Elstner,T.,Loebe,F.,Hagen,M.,\n",
      "Stein,B.,Potthast,M.:ContinuousIntegrationforReproducibleSharedTaskswith\n",
      "TIRA.io.AdvancesinInformationRetrieval.45thEuropeanConferenceonIRResearch\n",
      "(ECIR2023),Springer(2023)Overview of PAN 2024: Condensed Lab Overview 21\n",
      "[33] Gambini,M.,Tardelli,S.,Tesconi,M.:Theanatomyofconspiracytheorists:Unveiling\n",
      "traitsusingacomprehensivetwitterdataset.ComputerCommunications217,25–40(2024),\n",
      "https://doi.org/10.1016/j.comcom.2024.01.027\n",
      "[34] Gangopadhyay,S.,Khan,M.,Jabeen,H.:HybridDetox:CombiningSupervisedand\n",
      "UnsupervisedMethodsforEffectiveMultilingualTextDetoxification.WorkingNotesof\n",
      "CLEF2024,CEUR-WS.org(2024)\n",
      "[35] Giachanou,A.,Ghanem,B.,Rosso,P.:Detectionofconspiracypropagatorsusing\n",
      "psycho-linguisticcharacteristics.JournalofInformationScience49(1),3–17(2023),\n",
      "https://doi.org/10.1177/0165551520985486\n",
      "[36] Gómez-Romero,J.,González-Silot,S.,Montoro-Montarroso,A.,Molina-Solana,M.,\n",
      "MartínezCámara,E.:Detectionofconspiracy-relatedmessagesinTelegramwith\n",
      "anonymizednamedentities.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[37] Gritsai,G.,Boyeva,G.,Grabovoy,A.:Teamap-teamatPAN:LLMAdaptersforVarious\n",
      "Datasets.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[38] Guo,L.,Yang,W.,Ma,L.,Ruan,J.:BLGAV:GenerativeAIAuthorVerificationModel\n",
      "BasedonBERTandBiLSTM.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[39] Guo,M.,Han,Z.,Chen,H.,Peng,J.:AMachine-GeneratedTextDetectionModelBasedon\n",
      "TextMulti-FeatureFusion.WorkingNotesofCLEF2024,CEUR-WS.org(Sep2024)\n",
      "[40] Haddad,H.,Mulki,H.,Oueslati,A.:T-hsab:Atunisianhatespeechandabusivedataset.\n",
      "In:InternationalconferenceonArabiclanguageprocessing,pp.251–263,Springer(2019)\n",
      "[41] Halvani,O.,Winter,C.,Graner,L.:Ontheusefulnessofcompressionmodelsforauthorship\n",
      "verification.In:Proceedingsofthe12thInternationalConferenceonAvailability,Reliability\n",
      "andSecurity,vol.PartF1305,ACM,NewYork,NY,USA(29Aug2017),ISBN\n",
      "9781450352574,https://doi.org/10.1145/3098954.3104050\n",
      "[42] Hans,A.,Schwarzschild,A.,Cherepanova,V.,Kazemi,H.,Saha,A.,Goldblum,M.,\n",
      "Geiping,J.,Goldstein,T.:SpottingLLMswithBinoculars:Zero-shotdetectionof\n",
      "machine-generatedtext.arXiv[cs.CL](22Jan2024)\n",
      "[43] Hong,J.,Lee,N.,Thorne,J.:ORPO:monolithicpreferenceoptimizationwithoutreference\n",
      "model.CoRRabs/2403.07691(2024),https://doi.org/10.48550/ARXIV.2403.07691,URL\n",
      "https://doi.org/10.48550/arXiv.2403.07691\n",
      "[44] Hu,Q.,Han,Z.,Peng,J.,Guo,M.,Liu,C.:AnOppositionalThinkingAnalysisMethod\n",
      "UsingBERT-basedModelwithBiGRU.WorkingNotesofCLEF2024,CEUR-WS.org\n",
      "(2024)\n",
      "[45] Huang,B.,Zhong,C.,Yan,K.,Han,Y.:AuthorauthenticationofgenerativeAIbasedon\n",
      "BERTbyregularizationmethod.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[46] Huang,J.,Chen,Y.,Luo,M.,Li,Y.:GenerativeAIAuthorshipVerificationOfTri-Sentence\n",
      "AnalysisBaseOnTheBertModel.WorkingNotesofCLEF2024,CEUR-WS.org(Sep2024)\n",
      "[47] Huang,J.,Han,Z.,Zhu,R.,Guo,M.,Sun,K.:ConspiracyTheoryTextClassificationBased\n",
      "onCT-BERTandBETOModels.WorkingNotesofCLEF2024,CEUR-WS.org(Sep2024)\n",
      "[48] Huang,K.,Qi,H.,Yan,K.:Voight-KampffGenerativeAIAuthorshipVerificationbasedon\n",
      "ContrastiveLearningandDomainAdaptation.WorkingNotesofCLEF2024,\n",
      "CEUR-WS.org(Sep2024)\n",
      "[49] Huang,W.,Grieve,J.:AuthorialLanguageModelsForAIAuthorshipVerification.Working\n",
      "NotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[50] Huang,Z.,Kong,L.:TeamhuangzhijianatPAN:DeBERTa-v3withR-DropRegularization\n",
      "forMulti-AuthorWritingStyleAnalysis.WorkingNotesofCLEF2024,CEUR-WS.org\n",
      "(2024)\n",
      "[51] Huertas-García,Á.,Martí-González,C.,Muñoz,J.,Ambite,E.:SmallLanguageModelsand\n",
      "LargeLanguageModelsinOppositionalthinkinganalysis:CapabilitiesandBiasesand\n",
      "Challenges.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[52] Jiang,A.Q.,Sablayrolles,A.,Mensch,A.,Bamford,C.,Chaplot,D.S.,Casas,D.d.l.,\n",
      "Bressand,F.,Lengyel,G.,Lample,G.,Saulnier,L.,Lavaud,L.R.,Lachaux,M.A.,Stock,P.,\n",
      "Scao,T.L.,Lavril,T.,Wang,T.,Lacroix,T.,Sayed,W.E.:Mistral7B.arXiv[cs.CL]\n",
      "(10Oct2023)\n",
      "[53] Jigsaw:Toxiccommentclassificationchallenge.\n",
      "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge(2017),accessed:\n",
      "2024-03-18\n",
      "[54] Karlgren,J.,Dürlich,L.,Gogoulou,E.,Guillou,L.,Nivre,J.,Sahlgren,M.,Talman,A.:\n",
      "ELOQUENTCLEFSharedTasksforEvaluationofGenerativeLanguageModelQuality:\n",
      "46thEuropeanConferenceonInformationRetrieval(ECIR),SpringerNatureSwitzerland\n",
      "(2024),https://doi.org/10.1007/978-3-031-56069-9_63\n",
      "[55] Kestemont,M.,Tschuggnall,M.,Stamatatos,E.,Daelemans,W.,Specht,G.,Stein,B.,\n",
      "Potthast,M.:OverviewoftheAuthorIdentificationTaskatPAN2018:Cross-domain\n",
      "AuthorshipAttributionandStyleChangeDetection.In:WorkingNotesofCLEF2018,\n",
      "CEUR-WS.org(2018)\n",
      "[56] Khan,A.,Rai,M.,Khan,K.,Shah,S.,Alvi,F.,Samad,A.:TeamGladiatorsatPAN:\n",
      "ImprovingAuthorIdentification:AComparativeAnalysisofPre-TrainedTransformersfor\n",
      "Multi-AuthorClassification.WorkingNotesofCLEF2024,CEUR-WS.org(2024)22 Ayele et al.\n",
      "[57] King,J.,Baffour,P.,Crossley,S.,Holbrook,R.,Demkin,M.:Llm–detectaigeneratedtext\n",
      "(2023),URLhttps://kaggle.com/competitions/llm-detect-ai-generated-text\n",
      "[58] Koppel,M.,Schler,J.:Authorshipverificationasaone-classclassificationproblem.In:\n",
      "Twenty-firstinternationalconferenceonMachinelearning-ICML’04,pp.489–495,ACM\n",
      "Press,NewYork,NewYork,USA(2004),ISBN9781581138283,\n",
      "https://doi.org/10.1145/1015330.1015448\n",
      "[59] Koppel,M.,Winter,Y.:Determiningiftwodocumentsarewrittenbythesameauthor.\n",
      "JournaloftheAssociationforInformationScienceandTechnology65(1),178–187(2014)\n",
      "[60] Korenčić,D.,Chulvi,B.,Bonet,X.,Mariona,T.,Toselli,A.,Rosso,P.:Whatdistinguishes\n",
      "conspiracyfromcriticalnarratives?acomputationalanalysisofoppositionaldiscourse.\n",
      "expertsystems.ExpertSystem(2024)\n",
      "[61] Korenčić,D.,Chulvi,B.,BonetCasals,X.,Taulé,M.,Rosso,P.,Rangel,F.:Overviewof\n",
      "theoppositionalthinkinganalysispantaskatclef2024.In:Faggioli,G.,Ferro,N.,\n",
      "Galuščáková,P.,deHerrera,A.G.S.(eds.)WorkingNotesofCLEF2024–Conferenceand\n",
      "LabsoftheEvaluationForum(2024)\n",
      "[62] Korenčić,D.,Grubišić,I.,Toselli,A.H.,Chulvi,B.,Rosso,P.:TacklingCovid-19\n",
      "ConspiraciesonTwitterusingBERTEnsembles,GPT-3Augmentation,andGraphNNs.In:\n",
      "WorkingNotesProceedingsoftheMediaEval2022WorkshopBergen,NorwayandOnline\n",
      "(2023),URLhttps://2022.multimediaeval.com/paper8969.pdf\n",
      "[63] Ksi¸eżniak,E.,W¸ecel,K.,Sawiński,M.:TeamOpenFactatPAN2024:Fine-TuningBERT\n",
      "ModelswithStylometricEnhancements.WorkingNotesofCLEF2024,CEUR-WS.org\n",
      "(2024)\n",
      "[64] Kumar,S.,Balachandran,V.,Njoo,L.,Anastasopoulos,A.,Tsvetkov,Y.:Language\n",
      "generationmodelscancauseharm:Sowhatcanwedoaboutit?anactionablesurvey.\n",
      "CoRRabs/2210.07700(2022)\n",
      "[65] Lei,H.,Liu,X.,Niu,G.,Zhou,Y.,Zhou,Y.:GenerativeAIAuthorshipVerificationbased\n",
      "onChatGLM.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[66] Liang,X.,Lei,H.:Teamlxflcl66666atPAN:Fine-TunedReasoningforWritingStyle\n",
      "Analysis.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[67] Liang,X.,Wu,L.,Li,J.,Wang,Y.,Meng,Q.,Qin,T.,Chen,W.,Zhang,M.,Liu,T.:\n",
      "R-drop:Regularizeddropoutforneuralnetworks.In:34thAnnualConferenceonNeural\n",
      "InformationProcessingSystems2021,NeurIPS(2021)\n",
      "[68] Lin,T.,Wu,Y.,Lee,L.:TeamNYCU-NLPatPAN2024:IntegratingTransformerswith\n",
      "SimilarityAdjustmentsforMulti-AuthorWritingStyleAnalysis.WorkingNotesofCLEF\n",
      "2024,CEUR-WS.org(2024)\n",
      "[69] Lin,Z.,Han,Z.,Kong,L.,Chen,M.,Zhang,S.,Peng,J.,Sun,K.:AVerifyingGenerative\n",
      "TextAuthorshipModelWithRegularizedDropout.WorkingNotesofCLEF2024,\n",
      "CEUR-WS.org(2024)\n",
      "[70] Lin,Z.,Li,Y.,Huang,J.:Voight-KampffGenerativeAIAuthorshipVerificationBasedon\n",
      "T5.WorkingNotesofCLEF2024,CEUR-WS.org(Sep2024)\n",
      "[71] Liu,B.,Han,Z.,Cao,H.:AnApproachtoClassifyingConspiratorialandCriticalPublic\n",
      "HealthNarratives.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[72] Liu,C.,Han,Z.,Chen,H.,Hu,Q.:Teamliuc0757atPAN:AWritingStyleEmbedding\n",
      "MethodBasedonContrastiveLearningforMulti-AuthorWritingStyleAnalysis.Working\n",
      "NotesofCLEF2024,CEUR-WS.org(Sep2024)\n",
      "[73] Liu,X.,Chen,H.,Lv,J.:Teamfoshan-university-of-guangdongatPAN:Adaptive\n",
      "Entropy-BasedStability-PlasticityforMulti-AuthorWritingStyleAnalysis.WorkingNotes\n",
      "ofCLEF2024,CEUR-WS.org(Sep2024)\n",
      "[74] Liu,X.,Kong,L.:AITextDetectionMethodBasedonPerplexityFeatureswithStrided\n",
      "SlidingWindow.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[75] Logacheva,V.,Dementieva,D.,Ustyantsev,S.,Moskovskiy,D.,Dale,D.,Krotova,I.,\n",
      "Semenov,N.,Panchenko,A.:ParaDetox:Detoxificationwithparalleldata.In:Muresan,S.,\n",
      "Nakov,P.,Villavicencio,A.(eds.)Proceedingsofthe60thAnnualMeetingofthe\n",
      "AssociationforComputationalLinguistics(Volume1:LongPapers),pp.6804–6818,\n",
      "AssociationforComputationalLinguistics,Dublin,Ireland(May2022),\n",
      "https://doi.org/10.18653/v1/2022.acl-long.469,URL\n",
      "https://aclanthology.org/2022.acl-long.469\n",
      "[76] Lorenz,L.,Aygüler,F.Z.,Schlatt,F.,Mirzakhmedova,N.:BaselineAvengersatPAN2024:\n",
      "Often-ForgottenBaselinesforLLM-GeneratedTextDetection.WorkingNotesofCLEF\n",
      "2024,CEUR-WS.org(2024)\n",
      "[77] Lu,J.,Xu,B.,Zhang,X.,Min,C.,Yang,L.,Lin,H.:Facilitatingfine-graineddetectionof\n",
      "Chinesetoxiclanguage:Hierarchicaltaxonomy,resources,andbenchmarks.In:Rogers,A.,\n",
      "Boyd-Graber,J.,Okazaki,N.(eds.)Proceedingsofthe61stAnnualMeetingofthe\n",
      "AssociationforComputationalLinguistics,pp.16235–16250(Jul2023),URL\n",
      "https://aclanthology.org/2023.acl-long.898\n",
      "[78] Luo,Z.,Luo,M.,Wang,A.:MultilingualTextDetoxificationUsingGoogleCloud\n",
      "TranslationandPost-Processing.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[79] Lv,J.,Han,Y.,Kong,L.:Meta-ContrastiveLearningforGenerativeAIAuthorship\n",
      "Verification.WorkingNotesofCLEF2024,CEUR-WS.org(2024)Overview of PAN 2024: Condensed Lab Overview 23\n",
      "[80] Lv,J.,Yi,Y.,Qi,H.:TeamFosu-stuatPAN:Supervisedfine-tuningoflargelanguage\n",
      "modelsforMultiAuthorWritingStyleAnalysis.WorkingNotesofCLEF2024,\n",
      "CEUR-WS.org(Sep2024)\n",
      "[81] Mahesh,S.,Divakaran,S.,Girish,K.,Lakshmaiah,S.:BinaryBattle:LeveragingMLand\n",
      "TLModelstoDistinguishbetweenConspiracyTheoriesandCriticalThinking.Working\n",
      "NotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[82] Mandl,T.,Modha,S.,Majumder,P.,Patel,D.,Dave,M.,Mandlia,C.,Patel,A.:Overview\n",
      "ofthehasoctrackatfire2019:Hatespeechandoffensivecontentidentificationin\n",
      "indo-europeanlanguages.In:Proceedingsofthe11thAnnualMeetingoftheForumfor\n",
      "InformationRetrievalEvaluation,p.14–17,FIRE’19,ACM(2019),ISBN9781450377508,\n",
      "https://doi.org/10.1145/3368567.3368584\n",
      "[83] Mathet,Y.,Widlöcher,A.,Métivier,J.P.:TheUnifiedandHolisticMethodGammafor\n",
      "Inter-AnnotatorAgreementMeasureandAlignment.ComputationalLinguistics41(3),\n",
      "437–479(Sep2015),ISSN0891-2017,https://doi.org/10.1162/COLI_a_00227,URL\n",
      "https://doi.org/10.1162/COLI_a_00227\n",
      "[84] Miralles,P.,Martín,A.,Camacho,D.:EnsemblingNormalizedLogProbabilities.Working\n",
      "NotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[85] Mitchell,E.,Lee,Y.,Khazatsky,A.,Manning,C.D.,Finn,C.:DetectGPT:Zero-shot\n",
      "machine-generatedtextdetectionusingprobabilitycurvature.InternationalConferenceon\n",
      "MachineLearning202,24950–24962(26Jan2023),\n",
      "https://doi.org/10.48550/arXiv.2301.11305\n",
      "[86] MTS.AI:Cotype:Generativeaisolutions(2022),URLhttps://mts.ai,accessed:2024-05-31\n",
      "[87] Mubarak,H.,Darwish,K.,Magdy,W.,Elsayed,T.,Al-Khalifa,H.:Overviewofosact4\n",
      "arabicoffensivelanguagedetectionsharedtask.In:Proceedingsofthe4thWorkshopon\n",
      "open-sourcearabiccorporaandprocessingtools,withasharedtaskonoffensivelanguage\n",
      "detection,pp.48–52(2020)\n",
      "[88] Muennighoff,N.,Wang,T.,Sutawika,L.,Roberts,A.,Biderman,S.,Scao,T.L.,Bari,M.S.,\n",
      "Shen,S.,Yong,Z.X.,Schoelkopf,H.,Tang,X.,Radev,D.,Aji,A.F.,Almubarak,K.,\n",
      "Albanie,S.,Alyafeai,Z.,Webson,A.,Raff,E.,Raffel,C.:Crosslingualgeneralization\n",
      "throughmultitaskfinetuning.In:Proceedingsofthe61stACL,ACL(2023),\n",
      "https://doi.org/10.18653/V1/2023.ACL-LONG.891\n",
      "[89] Mulki,H.,Ghanem,B.:Let-mi:AnArabicLevantineTwitterdatasetformisogynistic\n",
      "language.In:Habash,N.,Bouamor,H.,Hajj,H.,Magdy,W.,Zaghouani,W.,Bougares,F.,\n",
      "Tomeh,N.,AbuFarha,I.,Touileb,S.(eds.)ProceedingsoftheSixthArabicNatural\n",
      "LanguageProcessingWorkshop,pp.154–163,AssociationforComputationalLinguistics,\n",
      "Kyiv,Ukraine(Virtual)(Apr2021),URLhttps://aclanthology.org/2021.wanlp-1.16\n",
      "[90] Mulki,H.,Haddad,H.,Ali,C.B.,Alshabani,H.:L-hsab:Alevantinetwitterdatasetfor\n",
      "hatespeechandabusivelanguage.In:Proceedingsofthethirdworkshoponabusive\n",
      "languageonline,pp.111–118(2019)\n",
      "[91] Najafi,M.,Tavan,E.,Colreavy,S.:MarsanatPAN2024TextDetox:ToxiCleanseRLand\n",
      "PavingtheWayforToxicity-FreeOnlineDiscourse.WorkingNotesofCLEF2024,\n",
      "CEUR-WS.org(2024)\n",
      "[92] OpenAI:Chatgpt:Optimizinglanguagemodelsfordialogue(2022),URL\n",
      "https://openai.com/blog/chatgpt,accessed:2024-05-31\n",
      "[93] Osipenko,M.,Korchagin,M.,Toleugazinov,A.,Egorov,S.,Udobang,J.:Fancy\n",
      "TransformersatPAN2024TextDetox:SurpassingtheBaselines.WorkingNotesofCLEF\n",
      "2024,CEUR-WS.org(2024)\n",
      "[94] Ostrower,B.,Wessell,J.,Bindal,A.:AIAuthorshipVerification:AnEnsembledApproach.\n",
      "WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[95] Peng,J.,Han,Z.,Zhang,H.,Ye,J.,Liu,C.,Liu,B.,Guo,M.,Chen,H.,Lin,Z.,Tang,Y.:\n",
      "AMultilingualTextDetoxificationMethodBasedonFew-shotLearningandCO-STAR\n",
      "Framework.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[96] Pereira-Kohatsu,J.C.,Sánchez,L.Q.,Liberatore,F.,Camacho-Collados,M.:Detectingand\n",
      "monitoringhatespeechintwitter.Sensors19(21),4654(2019),\n",
      "https://doi.org/10.3390/S19214654,URLhttps://doi.org/10.3390/s19214654\n",
      "[97] Pérez,J.M.,Furman,D.A.,AlonsoAlemany,L.,Luque,F.M.:RoBERTuito:apre-trained\n",
      "languagemodelforsocialmediatextinSpanish.Proceedingsofthe13thLREC,ELRA\n",
      "(2022),URLhttps://aclanthology.org/2022.lrec-1.785\n",
      "[98] Petropoulos,P.,Petropoulos,V.:RoBERTaandBi-LSTMforHumanvsAIgeneratedText\n",
      "Detection.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[99] Pletenev,S.:Memu_pro_kotowatPAN2024TextDetox:UncensoredLlama3Helpsto\n",
      "CensorBetter.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[100] Pogorelov,K.,Schroeder,D.T.,Brenner,S.,Langguth,J.:FakeNews:CoronaVirusand\n",
      "ConspiraciesMultimediaAnalysisTaskatMediaEval2021.In:WorkingNotesProceedings\n",
      "oftheMediaEval2021WorkshopBergen,NorwayandOnline(2021)\n",
      "[101] Pogorelov,K.,Schroeder,D.T.,Brenner,S.,Maulana,A.,Langguth,J.:Combiningtweets\n",
      "andconnectionsgraphforfakenewsdetectionatmediaeval2022.In:Proceedingsofthe\n",
      "MediaEval2022Workshop,Bergen,NorwayandOnline,12-13January2023.(2023)24 Ayele et al.\n",
      "[102] Protasov,V.:PAN2024MultilingualTextDetox:ExploringCross-lingualTransferinCase\n",
      "ofLargeLanguageModels.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[103] Qin,R.,Qi,H.,Yi,Y.:AmodelfusionapproachforgenerativeAIauthorshipverification.\n",
      "WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[104] Raffel,C.,Shazeer,N.,Roberts,A.,Lee,K.,Narang,S.,Matena,M.,Zhou,Y.,Li,W.,Liu,\n",
      "P.J.:Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.arXiv\n",
      "[cs.LG](23Oct2019)\n",
      "[105] Řehulka,E.,Šuppa,M.:RAGMeetsDetox:EnhancingTextDetoxificationUsing\n",
      "Open-SourceLargeLanguageModelswithRetrievalAugmentedGeneration.WorkingNotes\n",
      "ofCLEF2024,CEUR-WS.org(2024)\n",
      "[106] Risch,J.,Stoll,A.,Wilms,L.,Wiegand,M.:OverviewoftheGermEval2021SharedTask\n",
      "ontheIdentificationofToxic,Engaging,andFact-ClaimingComments.In:Proceedingsof\n",
      "theGermEval2021SharedTaskontheIdentificationofToxic,Engaging,andFact-Claiming\n",
      "Comments,pp.1–12,Duesseldorf,Germany(2021)\n",
      "[107] Ross,B.,Rist,M.,Carbonell,G.,Cabrera,B.,Kurowsky,N.,Wojatzki,M.:Measuringthe\n",
      "ReliabilityofHateSpeechAnnotations:TheCaseoftheEuropeanRefugeeCrisis.\n",
      "ProceedingsofNLP4CMCIII:3rdWorkshoponNaturalLanguageProcessingfor\n",
      "Computer-MediatedCommunication,BochumerLinguistischeArbeitsberichte,vol.17,pp.\n",
      "6–.9,Bochum,Germany(2016)\n",
      "[108] Rosso,P.,Rangel,F.,Potthast,M.,Stamatatos,E.,Tschuggnall,M.,Stein,B.:Overviewof\n",
      "PAN’16—NewChallengesforAuthorshipAnalysis:Cross-genreProfiling,Clustering,\n",
      "Diarization,andObfuscation.In:ExperimentalIRMeetsMultilinguality,Multimodality,\n",
      "andInteraction.7thInternationalConferenceoftheCLEFInitiative(CLEF16)(2016)\n",
      "[109] Ruffo,G.,Semeraro,A.,Giachanou,A.,Rosso,P.:Studyingfakenewsspreading,\n",
      "polarisationdynamics,andmanipulationbybots:Ataleofnetworksandlanguage.\n",
      "ComputerScienceReview47,100531(2023),ISSN1574-0137,\n",
      "https://doi.org/https://doi.org/10.1016/j.cosrev.2022.100531,URL\n",
      "https://www.sciencedirect.com/science/article/pii/S157401372200065X\n",
      "[110] Rykov,E.,Zaytsev,K.,Anisimov,I.,Voronin,A.:SmurfCatatPANTexDetox2024:\n",
      "AlignmentofMultilingualTransformersforTextDetoxification.WorkingNotesofCLEF\n",
      "2024,CEUR-WS.org(2024)\n",
      "[111] Sahitaj,A.,Sahitaj,P.,Mohtaj,S.,Möller,S.,Schmitt,V.:TowardsaComputational\n",
      "FrameworkforDistinguishingCriticalandConspiratorialTextsbyElaboratingonthe\n",
      "ContextandArgumentationwithLLMs.WorkingNotesofCLEF2024,CEUR-WS.org\n",
      "(2024)\n",
      "[112] Sánchez-Hermosilla,I.,PanizoLledot,A.,Camacho,D.:AStudyonNLPModelEnsembles\n",
      "andDataAugmentationTechniquesforSeparatingCriticalThinkingfromConspiracy\n",
      "TheoriesinEnglishTexts.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[113] Sanjesh,R.,Mangai,A.:TeamriyasanjeshatPAN:Multi-featurewithCNNandBi-LSTM\n",
      "NeuralNetworkapproachtoStyleChangeDetection.WorkingNotesofCLEF2024,\n",
      "CEUR-WS.org(2024)\n",
      "[114] Sculley,D.,Brodley,C.E.:Compressionandmachinelearning:Anewperspectiveonfeature\n",
      "spacevectors.In:DataCompressionConference(DCC’06),pp.332–341,IEEE(2006),ISBN\n",
      "9780769525457,ISSN1068-0314,2375-0359,https://doi.org/10.1109/dcc.2006.13\n",
      "[115] Semiletov,A.:ToxicRussianComments:LabelledcommentsfromthepopularRussian\n",
      "socialnetwork.https://www.kaggle.com/alexandersemiletov/toxic-russian-comments(2020),\n",
      "accessed:2023-12-14\n",
      "[116] Sheykhlan,M.,Abdoljabbar,S.,Mahmoudabad,M.:Teamkarami-kheiriatPAN:\n",
      "EnhancingMachine-GeneratedTextDetectionwithEnsembleLearningBasedon\n",
      "TransformerModels.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[117] Sheykhlan,M.,Abdoljabbar,S.,Mahmoudabad,M.:Teamkarami-shatPAN:\n",
      "Transformer-basedEnsembleLearningforMulti-AuthorWritingStyleAnalysis.Working\n",
      "NotesofCLEF2024,CEUR-WS.org(Sep2024)\n",
      "[118] Stamatatos,E.,Kestemont,M.,Kredens,K.,Pezik,P.,Heini,A.,Bevendorff,J.,Potthast,\n",
      "M.,Stein,B.:OverviewoftheAuthorshipVerificationTaskatPAN2022.In:CLEF2022\n",
      "LabsandWorkshops,CEUR-WS.org(2022)\n",
      "[119] Stamatatos,E.,Potthast,M.,Pardo,F.M.R.,Rosso,P.,Stein,B.:Overviewofthe\n",
      "PAN/CLEF2015evaluationlab.ExperimentalIRMeetsMultilinguality,Multimodality,\n",
      "andInteraction-6thInternationalConferenceoftheCLEFAssociation,CLEF2015,\n",
      "Springer(2015)\n",
      "[120] Su,J.,Zhuo,T.Y.,Wang,D.,Nakov,P.:DetectLLM:Leveraginglogrankinformationfor\n",
      "zero-shotdetectionofmachine-generatedtext.arXiv[cs.CL](23May2023)\n",
      "[121] Su,Y.,Lan,T.,Wang,Y.,Yogatama,D.,Kong,L.,Collier,N.:Acontrastiveframeworkfor\n",
      "neuraltextgeneration.arXiv[cs.CL](13Feb2022)\n",
      "[122] Sun,G.,Yang,W.,Ma,L.:BCAV:AGenerativeAIAuthorVerificationModelBasedon\n",
      "theIntegrationofBertandCNN.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[123] Sushko,N.:PAN2024MultilingualTextDetox:ExploringDifferentRegimesForSynthetic\n",
      "DataTrainingForMultilingualTextDetoxification.WorkingNotesofCLEF2024,\n",
      "CEUR-WS.org(2024)Overview of PAN 2024: Condensed Lab Overview 25\n",
      "[124] Taulé,M.,Nofre,M.,Bargiela,V.,Bonet,X.:Newscom-tox:acorpusofcommentsonnews\n",
      "articlesannotatedfortoxicityinspanish.LREC(2024)\n",
      "[125] Tavan,E.,Najafi,M.:MarsanatPAN:BinocularLLMandFusingBinoculars’Insightwith\n",
      "theProficiencyofLargeLanguageModelsforCutting-EdgeMachine-GeneratedText\n",
      "Detection.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[126] Tian,Y.,Chen,H.,Wang,X.,Bai,Z.,Zhang,Q.,Li,R.,Xu,C.,Wang,Y.:Multiscale\n",
      "positive-unlabeleddetectionofai-generatedtexts.CoRRabs/2305.18149(2023),\n",
      "https://doi.org/10.48550/ARXIV.2305.18149\n",
      "[127] Tschuggnall,M.,Stamatatos,E.,Verhoeven,B.,Daelemans,W.,Specht,G.,Stein,B.,\n",
      "Potthast,M.:OverviewoftheauthoridentificationtaskatPAN2017:stylebreach\n",
      "detectionandauthorclustering.In:CLEF2017LabsandWorkshops(2017)\n",
      "[128] Tulbure,A.,CollArdanuy,M.:Conspiracyvscriticalthinkingusinganensembleof\n",
      "transformerswithdataaugmentationtechniques.WorkingNotesofCLEF2024,\n",
      "CEUR-WS.org(2024)\n",
      "[129] Valdez-Valenzuela,A.,Gómez-Adorno,H.:TeamiimasnlpatPAN:LeveragingGraph\n",
      "NeuralNetworksandLargeLanguageModelsforGenerativeAIAuthorshipVerification.\n",
      "WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[130] Vallecillo-Rodríguez,M.,Martín-Valdivia,A.M.:SINAIatPAN2024TextDetox:\n",
      "ApplicationofTreeofThoughtStrategyinLargeLanguageModelsforMultilingualText\n",
      "Detoxification.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[131] Vallecillo-Rodríguez,M.,Martín-Valdivia,M.,Montejo-Ráez,A.:SINAIatPAN2024\n",
      "OppositionalThinkingAnalysis:Exploringthefine-tuningperformanceofLLMs.Working\n",
      "NotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[132] Weimer,A.M.,Barth,F.,Dönicke,T.,Gödeke,L.,Varachkina,H.,Holler,A.,Sporleder,C.,\n",
      "Gittel,B.:The(In-)ConsistencyofLiteraryConcepts.Operationalising,Annotatingand\n",
      "DetectingLiteraryComment.JournalofComputationalLiteraryStudies1(1)(Dec2022),\n",
      "ISSN2940-1348,https://doi.org/10.48694/jcls.90,URLhttps://jcls.io/article/id/90/,\n",
      "number:1Publisher:Universitäts-undLandesbibliothekDarmstadt\n",
      "[133] Wiegand,M.,Siegel,M.,Ruppenhofer,J.:OverviewoftheGermEval2018SharedTaskon\n",
      "theIdentificationofOffensiveLanguage(2018)\n",
      "[134] Wu,B.,Han,Y.,Yan,K.,Qi,H.:TeambakeratPAN:EnhancingWritingStyleChange\n",
      "DetectionwithVirtualSoftmax.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[135] Wu,Q.,Kong,L.,Ye,Z.:TeambingezzzleepatPAN:AWritingStyleChangeAnalysis\n",
      "ModelBasedonRoBERTaEncodingandContrastiveLearningforMulti-AuthorWriting\n",
      "StyleAnalysis.WorkingNotesofCLEF2024,CEUR-WS.org(Sep2024)\n",
      "[136] Wu,Z.,Yang,W.,Ma,L.,Zhao,Z.:BertT:AHybridNeuralNetworkModelforGenerative\n",
      "AIAuthorshipVerification.WorkingNotesofCLEF2024,CEUR-WS.org(Sep2024)\n",
      "[137] Xue,L.,Constant,N.,Roberts,A.,Kale,M.,Al-Rfou,R.,Siddhant,A.,Barua,A.,Raffel,\n",
      "C.:mt5:Amassivelymultilingualpre-trainedtext-to-texttransformer.Proceedingsofthe\n",
      "NAACL-HLT2021,ACL,https://doi.org/10.18653/V1/2021.NAACL-MAIN.41,URL\n",
      "https://doi.org/10.18653/v1/2021.naacl-main.41\n",
      "[138] Yadagiri,A.,Kalita,D.,Ranjan,A.,Bostan,A.,Toppo,P.,Pakray,P.:Teamcnlp-nits-pp\n",
      "atPAN:LeveragingBERTforAccurateAuthorshipVerification:ANovelApproachto\n",
      "TextualAttribution.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[139] Ye,Z.,Zhong,Y.,Huang,C.,Kong,L.:Teamno-999atPAN:ContinualTransferLearning\n",
      "withProgressPromptforMulti-AuthorWritingStyleAnalysis\".WorkingNotesofCLEF\n",
      "2024,CEUR-WS.org(2024)\n",
      "[140] Ye,Z.,Zhong,Y.,Huang,Z.,Kong,L.:TokenPredictionasImplicitClassificationfor\n",
      "GenerativeAIAuthorshipVerification.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[141] Zangerle,E.,Mayerl,M.,,Potthast,M.,Stein,B.:OverviewoftheStyleChangeDetection\n",
      "TaskatPAN2021.In:Faggioli,G.,Ferro,N.,Joly,A.,Maistro,M.,Piroi,F.(eds.)CLEF\n",
      "2021LabsandWorkshops,CEUR-WS.org(2021)\n",
      "[142] Zangerle,E.,Mayerl,M.,,Potthast,M.,Stein,B.:OverviewoftheStyleChangeDetection\n",
      "TaskatPAN2022.In:CLEF2022LabsandWorkshops,CEUR-WS.org(2022)\n",
      "[143] Zangerle,E.,Mayerl,M.,,Potthast,M.,Stein,B.:OverviewoftheStyleChangeDetection\n",
      "TaskatPAN2023.In:CLEF2023LabsandWorkshops,CEUR-WS.org(2023)\n",
      "[144] Zangerle,E.,Mayerl,M.,Potthast,M.,Stein,B.:OverviewoftheMulti-AuthorWriting\n",
      "StyleAnalysisTaskatPAN2024.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[145] Zangerle,E.,Mayerl,M.,Specht,G.,Potthast,M.,Stein,B.:OverviewoftheStyleChange\n",
      "DetectionTaskatPAN2020.In:CLEF2020LabsandWorkshops(2020)\n",
      "[146] Zangerle,E.,Tschuggnall,M.,Specht,G.,Stein,B.,Potthast,M.:OverviewoftheStyle\n",
      "ChangeDetectionTaskatPAN2019.In:CLEF2019LabsandWorkshops(2019)\n",
      "[147] Zeng,Z.,Han,Z.,Ye,J.,Tan,Y.,Cao,H.,Li,Z.,Huang,R.:AConspiracyTheoryText\n",
      "DetectionMethodbasedonRoBERTaandXLM-RoBERTaModels.WorkingNotesof\n",
      "CLEF2024,CEUR-WS.org(2024)\n",
      "[148] Zhu,Y.,Kong,L.:AIAuthorshipVerificationBasedOnDebertaModel.WorkingNotesof\n",
      "CLEF2024,CEUR-WS.org(2024)\n",
      "[149] Zinkovich,V.,Karpukhin,S.,Kurdiukov,N.,Tikhomirov,P.:nlp_enjoyersatMultilingual\n",
      "TextualDetoxification(CLEF-2024.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n",
      "[150] Zrnić,L.:Conspiracytheorydetectionusingtransformerswithmulti-taskandmultilingual\n",
      "approaches.WorkingNotesofCLEF2024,CEUR-WS.org(2024)\n"
     ]
    }
   ],
   "source": [
    "#Example usage\n",
    "pdf_file_path = \"pan_2024.pdf\"\n",
    "pdf_text = extract_text_from_pdf_with_plumber(pdf_file_path)\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76663"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
